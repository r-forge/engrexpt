% NOTE -- ONLY EDIT THE .Rnw FILE!!!  The .tex file is,
% likely to be overwritten.
\usepackage{SweaveSlides}
\title[Chapter 5]{Chapter 5: Inference for a single population}
%\subject{Models}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=10,height=6.5,strip.white=all,keep.source=TRUE}
\SweaveOpts{prefix=TRUE,prefix.string=figs/ch05,include=TRUE}
\begin{document}
\begin{frame}
  \titlepage
\end{frame}
\begin{frame}
  \frametitle{Outline}
  \tableofcontents[pausesections]
\end{frame}
<<preliminaries,echo=FALSE,results=hide>>=
options(width=60)
library(EngrExpt)
options(show.signif.stars = FALSE)
#lattice.options(default.theme = function() standard.theme(color=FALSE))
@ 
\newcommand{\rvu}{\ensuremath{\mathcal{U}}}
\newcommand{\rvw}{\ensuremath{\mathcal{W}}}
\newcommand{\rvx}{\ensuremath{\mathcal{X}}}
\newcommand{\rvy}{\ensuremath{\mathcal{Y}}}
\newcommand{\rvz}{\ensuremath{\mathcal{Z}}}

\section[5.1 Central Limit Thm.]{5.1 Central Limit Theorem}
\begin{frame}
  \frametitle{The Central Limit Theorem}
  \begin{itemize}
  \item The central limit theorem is one of the most important results
    in mathematical statistics.  It says that the sample means from a
    \Emph{random sample} (meaning independent samples from a stable
    process) will be normally distributed, regardless of what the
    original distribution was, when $n$ is sufficiently large.
  \item Formally, if $\rvy_1,\rvy_2,\dots,\rvy_n$ is a random sample
    from a distribution with $\sigma^2<\infty$ then for large samples,
    $\bar{\rvy}$ is approximately normally distributed.
  \item This is a remarkably powerful result; first, because it is
    very general and secondly because it is a description of the
    asymptotic or ``limiting'' distribution but it holds for quite
    small values of $n$.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Other properties of the distribution of the sample mean}
  \begin{itemize}
  \item If the random variables $\rvy_1,\rvy_2,\dots,\rvy_n$ are a
    random sample (sometimes also described as a ``independent and
    identically distributed'' or i.i.d. sample) from a distribution
    with mean $\mu$ and variance $\sigma^2$ then
    $\mathrm{E}(\bar{\rvy})=\mu$ and $\mathrm{Var}(\bar{\rvy}) =
    \sigma^2/n$.
  \item So the central limit theorem states that, for large $n$,
    \begin{displaymath}
      \bar{\rvy}\sim\mathcal{N}\left(\mu,\frac{\sigma^2}{n}\right)
    \end{displaymath}
  \item Exactly how large $n$ must be depends on the form of the
    original distribution.  If it is continuous and reasonably
    symmetric then $n=15$ may be large enough.  If it is skewed but
    continuous we may need $n=30$ or more.  For discrete and skewed we
    may need as much as $n=100$.
  \item Although in practice we only have one sample and one average,
    $\bar{y}$ we can use computer simulation to consider the sorts of
    samples we could have gotten and the distribution of the statistic.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conducting a simulation study (not part of the course)}
  \begin{itemize}
  \item Suppose we wish to simulate the value of a statistic
    (e.g. mean or median or variance or standard deviation) from
    samples of size $n$ drawn from a certain distribution.  Let $K$ be
    the number of replicates we want to obtain.
  \item The \Emph{sample size}, $n$, is typically small.  The number
    of replicates, $K$, can be very large.  The larger the value of
    $K$, the more accurately we can determine the distribution of the
    statistic.  With modern computers we can afford to use values of
    $K$ in the hundreds of thousands or more.
  \end{itemize}
\begin{itemize}
\item First determine how to evaluate the statistic from a single
  sample of size $n$ then use the \code{replicate} function to
  repeat the process $K$ times.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Mean of samples of size 5 from U(-1,1)}
  What is the shape of the distribution of the mean of a sample of
  size $n=5$ from a $U(-1,1)$ distribution?
<<umeansim>>=
mns5 <- replicate(50000, mean(runif(5, min = -1, max = 1)))
@ 
<<umeanshistshow,eval=FALSE>>=
histogram(~mns5,breaks = seq(-1, 1, len = 40))
@ 
\begin{center}
<<umeanshist,fig=TRUE,echo=FALSE,height=4>>=
print(histogram(~mns5,xlab=NULL,breaks = seq(-1, 1, len = 40)))
@ 
\end{center}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Sampling densities of statistics}
  \begin{itemize}
  \item The idiom 
    \begin{center}
      \code{replicate(K, <statfn>(r<distab>(n, <pars>)))}
    \end{center}
    produces \code{K} replicates of the statistic calculated by
    \code{<statfn>} (examples are \code{mean}, \code{median},
    \code{var} and \code{sd}) on samples of size \code{n} from
    distribution \code{<distab>} with parameter(s) \code{<pars>}.
  \item Typically \code{K} is large and \code{n} is small.  Values of
    10,000 or 100,000 are used for \code{K} on modern computers.  The
    larger the value of \code{K} the smoother the approximation to the
    sampling density.  \code{n} is the size of the actual sample 
    you can afford to collect.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Effect of changing the sample size, $n$}
  \begin{itemize}
  \item Performing multiple simulations allows us to see how
    characteristics of the distribution of $\bar{\rvy}$ depends on $n$.
  \end{itemize}
<<moremeans>>=
mns1 <- runif(50000, -1, 1)
mns10 <- replicate(50000, mean(runif(10, -1, 1)))
mns20 <- replicate(50000, mean(runif(20, -1, 1)))
sapply(list(mns1, mns5, mns10, mns20), mean)
sapply(list(mns1, mns5, mns10, mns20), var)
@ 
\begin{itemize}
\item As $n$ increases the expected value of the sample mean stays
  near 0.
\item As $n$ increases the variance of the sample mean decreases.
  Roughly, $V(\bar{X}_n)=\frac{1}{3}\cdot\frac{1}{n}$
\end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Shape of distribution of $\bar{X}_n$}
  \begin{itemize}
  \item As $n$ increases, the shape of the distribution of $\bar{X}_n$
    tends to the ``bell-curve'' or Gaussian shape and it has less
    variability.  That is, it tends to a ``central limit''.
  \end{itemize}
  \begin{center}
<<mnshist,fig=TRUE,echo=FALSE,height=5.5>>=
print(histogram(~ mns1 + mns5 + mns10 + mns20, outer = TRUE,
                breaks = seq(-1,1,len = 40), layout = c(4,1),
                xlab = "Means of samples of size n from U(-1,1)"))
@     
  \end{center}
\end{frame}
\begin{frame}
  \frametitle{More detail on the shape of the distribution of $\bar{\rvy}$}
  \begin{itemize}
  \item In addition to the histogram we can use normal probability
    plots to evaluate the deviations of the distribution of
    $\bar{\rvy}$ from normality.
  \end{itemize}
  \begin{center}
<<qqmathrunif,fig=TRUE,echo=FALSE,height = 4>>=
print(qqmath(~ mns1 + mns5 + mns10 + mns20, outer = TRUE,
             f.value = ppoints(200), layout = c(4,1), ylab = NULL,
             xlab = "Means of samples of size n from U(-1,1)"))

@     
  \end{center}
\end{frame}
\begin{frame}
  \frametitle{Overlaid normal probability plots for $\bar{\rvy}_n$}
  \begin{center}
<<qqmathoverlaid,fig=TRUE,echo=FALSE,height=5.5>>=
show(qqmath(~ mns1 + mns5 + mns10 + mns20, outer = FALSE, type = c("g","l"),
            ylab = NULL, aspect = 1, f.values = ppoints(200),
            xlab = "Standard normal quantiles",
            auto.key = list(columns = 4, lines = TRUE, points = FALSE)))
@     
  \end{center}
The conclusion is that the distribution of means from an i.i.d. sample
of a uniform distribution is very close to a normal, even for $n=5$.
\end{frame}
\begin{frame}[fragile]
  \frametitle{Sample means from an exponential distribution}
<<emns>>=
emns01 <- replicate(50000, mean(rexp(1, rate = 1/7)))
emns05 <- replicate(50000, mean(rexp(5, rate = 1/7)))
emns15 <- replicate(50000, mean(rexp(15, rate = 1/7)))
emns50 <- replicate(50000, mean(rexp(50, rate = 1/7)))
@   
  \begin{center}
<<eqqmathoverlaid,fig=TRUE,echo=FALSE,height=4.5>>=
show(qqmath(~ emns01 + emns05 + emns15 + emns50,
            outer = TRUE, type = c("g","l"),
            ylab = NULL, aspect = 1, f.values = ppoints(200),
            xlab = "Standard normal quantiles",
            scales = list(y = list(relation = "free"))))
@     
  \end{center}
  Even for $n=50$ there is noticeable skewness in the distribution
  (althought we would not be far wrong in assuming normality at
  $n=50$).
\end{frame}
\begin{frame}[fragile]
  \frametitle{Elementary uses of the C.L.T.}
  \begin{itemize}
  \item If we have plausible values of the variance of our process,
    perhaps from a pilot study, we can use the normal distribution and
    the Central Limit Theorem (C.L.T.) to evaluate probabilities
    regarding the sample mean.
  \item Example 5.1.3 discusses product lifetimes that have an unknown
    mean and a variance of approximately 8 years.  The number of
    products to sample so that we are 95\% certain that $\bar{y}$ will
    be within 1 year of the true mean is derived from
    \begin{displaymath}
      \begin{aligned}
        0.95&=P(|\bar{\rvy}-\mu|<1)
      \end{aligned}
    \end{displaymath}
    The distribution of $\bar{\rvy}$ will be approximately normal with
    mean $\mu$ and standard deviation $\sigma/\sqrt{n}$.  For a
    standard normal, 95\% of the probability is within ``2'' standard
    deviations of the mean (the actual multiple is \code{qnorm(0.025)=
      \Sexpr{round(qnorm(0.025), 5)}}) so we want
    $1=\mathtt{qnorm}(0.025)^2\frac{8}{n}$.  That is, $n >$
  \end{itemize}
<<>>=
8 * qnorm(0.025)^2
@ 
\end{frame}
\begin{frame}
  \frametitle{Approximations for binomial or Poisson distributons}
  \begin{itemize}
  \item The text describes approximations of the probabilities for a
    binomial or Poisson distribution based on the normal distributon.
  \item These are interesting from the point of view of understanding
    that these distributions will tend to have a ``bell-curve'' shape
    when $n$ is large and $p$ is moderate for the binomial or
    $\lambda\,t$ is large for the Poisson.
  \item In practice, though, you can evaluate probabilities for such
    distributions exactly so there is no need to use approximations.
  \end{itemize}
\end{frame}

\section[5.2 Conf. int. for $\mu$]{5.2 A confidence interval for $\mu$}
\begin{frame}
  \frametitle{Confidence intervals}
  \begin{itemize}
  \item Our ``best guess'' at a parameter is called a \Emph{point
      estimate}.  For example, we usually use the sample mean,
    $\bar{y}$, as the point estimate of $\mu$.
  \item An \Emph{interval estimate} or \Emph{confidence interval} is
    an interval of plausible values for the parameter.  Values outside
    the interval are ``unreasonable'' and values inside are ``not
    unreasonable''.
  \item To calibrate the meaning of ``unreasonable'' we assign a value
    $\alpha$ to the probability of getting data like we did or even
    more extreme when the parameter is outside.  This corresponds to
    the ``p-value'' in a hypothesis test.
  \item The \Emph{coverage probability} or \Emph{confidence level} is
    $1-\alpha$.  Typically we set $\alpha=0.05$ or $\alpha=0.01$
    resulting in 95\% or 99\% confidence intervals.
  \item Formally, the coverage probability is the probability that an
    interval constructed in this way will cover the true parameter
    value.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{A confidence interval on $\mu$}
  \begin{itemize}
  \item In the unlikely event that someone were to tell us what the
    standard deviation, $\sigma$, of the population was but somehow
    not know much about the mean, $\mu$, we could create a
    $(1-\alpha)$ confidence interval as
    \begin{displaymath}
      \bar{y}\pm z(\alpha/2)\frac{\sigma}{n}
    \end{displaymath}
    where $z(\alpha/2)$ is the \textbf{upper} $\alpha/2$ quantile of
    the standard normal distribution.
  \item For example, the upper 0.025 quantile of the standard normal is
<<qnorm025>>=
qnorm(0.025, low = FALSE)
@ 
so a 95\% confidence interval on $\mu$ for this artificial, ``known
sigma'' case is
\begin{displaymath}
  \bar{y}\pm1.959964 \frac{\sigma}{\sqrt{n}}
\end{displaymath}
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Use of Student's T distribution}
  \begin{itemize}
  \item In the real world no one tells us what $\sigma$ is and we must
    estimate it as $s$.  A statistician named William Gossett, who
    wrote under the pseudonym ``A Student'', derived the distribution
    of the shifted, scaled sample mean when the scale is based on the
    estimate, $s$, not the theoretical value $\sigma$.
  \item This distribution is called the ``Student's t distribution''.
    It is similar to the standard normal distribution but a bit more
    spread out.  The spreading depends on the number of
    ``degrees of freedom'' in the estimate of $\sigma^2$.  The degrees
    of freedom are written as $\nu$.  For a single sample $\nu=n-1$.
  \item As $\nu$ increases the T distribution approaches the standard
    normal.  If we were using tables we would call anything with
    $\nu>30$ a standard normal.  When using a computer we don't
    bother.
  \item Notation: the t distribution with $\nu$ degrees of freedom is
    written $t(\nu)$.  The corresponding \R{} functions are \code{dt},
    \code{pt}, \code{qt} and \code{rt}.  The upper $\alpha$ quantile
    is written $t(\alpha;\nu)$.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Graphical comparison of $t(\nu)$ and $\rvz$}
  \begin{center}
<<tdensity,fig=TRUE,echo=FALSE>>=
xv <- seq(-4.5, 4.5, 0.01)
print(xyplot(dnorm(xv)+dt(xv,25)+dt(xv,10)+dt(xv,5) ~ xv,
             scales = list(x = list(axs = "i")),
             ylab = NULL, xlab = NULL, type = c("g", "l"),
             auto.key = list(text = expression(Z, T[25], T[10], T[5]),
             points = FALSE, lines = TRUE, columns = 4)))
@ 
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{General form of the confidence interval}
  \begin{itemize}
  \item The general form of the confidence interval on $\mu$ is
    \begin{displaymath}
      \bar{y}\pm t\left(\frac{\alpha}{2}, n - 1\right)\frac{s}{\sqrt{n}}
    \end{displaymath}
  \item We can use this formula for any values of $n$.  If $n$ is
    large we don't need strong assumptions on the shape of the
    original distribution.  If $n$ is small we must assume that the
    original distribution is close to the normal (but, of course, we
    can't check this with a small sample - a ``Catch 22'' situation).
  \item The \R function to create this interval is \code{t.test}.  The
    name comes from the corresponding hypothesis test, which we will
    discuss later.
  \end{itemize}
\end{frame}
<<opdig,echo=FALSE,results=hide>>=
op <- options(digits=5)
@ 
\begin{frame}[fragile]
  \frametitle{Example 5.2.2}

  The example provides (probably fictitious) discharge times for a
  particular electric vehicle
<<times>>=
sd(charge <- c(5.11,2.1,4.27,5.04,4.47,3.73,5.96,6.21))
summary(charge)
t.test(charge)
@   
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example 5.2.2 (cont'd)}

  Because the degrees of freedom, $\nu=7$, are quite small we should
  check for normality.
  \begin{center}
<<chargeplot,fig=TRUE,echo=FALSE,height=4.5>>=
print(densityplot(charge,xlab = "Discharge times"),
      pos = c(0,0,0.65,1), more = TRUE)
print(qqmath(charge,type=c("g","p"), aspect=1, ylab = NULL,
             xlab = NULL), pos = c(0.65,0,1,1))
@   
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Another \R{} evaluation of confidence intervals}
  \begin{itemize}
  \item Another way of evaluating a confidence intervals on $\mu$ is
    with the \code{confint} function, which provides confidence
    intervals on the parameters in a fitted model.
  \item To use \code{confint} we fit what we sometimes call the
    ``trivial'' model
    \begin{displaymath}
      \rvy_i = \mu + \epsilon_i,\quad i=1,\dots,n
    \end{displaymath}
    The estimate of $\mu$, $\widehat{\mu}=\bar{y}$ will be called
    \code{(Intercept)} in the output.  The formula for the model
    contains the constant term, \code{1}, as the only predictor.
  \end{itemize}
<<confint1>>=
confint(fm1 <- lm(charge ~ 1))
@   
\end{frame}
\begin{frame}[fragile]
  \frametitle{Clear-coat thickness (example 5.2.4)}
\begin{center}
<<thickplot,fig=TRUE,width=12,height=3.5,echo=FALSE>>=
print(qqmath(~ thickness, ccthickn, aspect = 1,
             xlab = "Standard normal quantiles"),
      pos = c(0,0,0.40,1), more = TRUE)
print(densityplot(~thickness, ccthickn),
      pos = c(0.40,0,1,1))
@   
\end{center}
<<assaydat>>=
with(ccthickn, summary(thickness))
sd(ccthickn$thickness)
confint(fm2 <- lm(thickness ~ 1, ccthickn))
@ 
\end{frame}
<<unopt,echo=FALSE,results=hide>>=
options(op)
@ 
\begin{frame}[fragile]
  \frametitle{Clear-coat thickness (cont'd)}

  The summary of the fit of the ``trivial'' model includes many of the
  statistics from the data.
<<fm2sumshow,eval=FALSE>>=
summary(fm2)
@ 
<<fm2sum,echo=FALSE>>=
cat(paste(capture.output(summary(fm2))[-(1:9)], "", sep="\n"))
@ 
The values in this summary include
\begin{description}
\item[$\bar{y}=64.26$] The parameter estimate, $\hat{\mu}$.
\item[$s=2.718$] The sample standard deviation, $\hat{\sigma}$
\item[$n-1=39$] The degrees of freedom, $\nu$, for the variance estimate, $s^2$.
\item[$\frac{s}{\sqrt{n}}=0.4297$] The standard error of the mean, $\sqrt{\textrm{Var}(\bar{\rvy})}$
\end{description}

\end{frame}
\begin{frame}
  \frametitle{Sample sizes}
  \begin{itemize}
  \item The half-width of a confidence interval, also called the
    \Emph{margin of error} depends on
    \begin{description}
    \item[The confidence level] Higher confidence levels require wider
      intervals
    \item[The standard deviation] More variability in the original
      distribution results in wider intervals.
    \item[The sample size] Larger samples produce narrower intervals.
    \end{description}
  \item Given a working value for $\sigma$ we can determine the sample
    size needed to attain a given margin of error.
  \item If we are willing to assume that $n$ is large we can use
    $z(\alpha/2)$ in the calculation.  For small $n$ it gets tricky
    because $\nu=n-1$ determines the multiplier which, in turn, affects
    the sample size. We must solve a nonlinear equation but
    computers are good at that. 
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Sample size calculations}
  \begin{itemize}
  \item Example 5.2.5 shows calculations for the sample size from the formula
      $n = \left[\frac{t(\alpha/2;\infty)s}{d}\right]^2$
    when the desired margin of error, $d$, is 0.2, the working value
    of $s$ is 0.4 and $\alpha$ is 5\% and we round the answer to the
    next largest integer.
<<sampsz>>=
ceiling((qnorm(0.025)*0.4/0.2)^2)
@     
\item Because this is a small value of $n$ we should instead solve for
  $n$ in $n = \left[\frac{t(\alpha/2;n-1)s}{d}\right]^2$
\end{itemize}
<<sampreal>>=
ceiling(uniroot(function(x) x-(qt(.025,x-1)*0.4/0.2)^2,
                c(2,100))$root)
@ 
\end{frame}

\section[5.3 Prediction int.]{5.3 Prediction and tolerance intervals}

\begin{frame}
  \frametitle{Section 5.3: Prediction and tolerance intervals}
  \begin{itemize}
  \item A confidence interval on $\mu$ provides a measure of the
    precision of the information regarding the unknown population
    parameter.  It does not directly tell us about bounds on where we
    expect a future observation to fall.
  \item A \Emph{prediction interval} indicates where a single future
    observation is likely to be.
  \item A \Emph{tolerance interval} indicates where a large proportion
    of the population is likely to be.
  \item Unlike the confidence interval on $\mu$, prediction intervals
    and tolerance intervals depend strongly on the shape of the
    distribution of the data.
  \item In theory one can make a confidence interval arbitrarily
    narrow by taking a sufficiently large sample.  You can't do this
    for a prediction interval.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Prediction intervals on a future observation}
  \begin{itemize}
  \item If it is reasonable to assume that the \Emph{data}
    (i.e. $\rvy_1,\rvy_2,\dots,\rvy_n$) are from normal distribution
    then we could say that a model for the data is
    \begin{displaymath}
      \rvy_i=\mu+\epsilon_i,\quad \epsilon_i\sim\mathcal{N}(0,\sigma^2)
    \end{displaymath}
  \item Our estimate $\widehat{\mu}=\bar{\rvy}_n$ is independent of
    $\epsilon_{n+1}$.  The variability in the difference between
    $\rvy_{n+1}$ and $\bar{\rvy}_n$ is the sum of the variability in
    $\bar{\rvy}_n-\mu$ ($\frac{\sigma^2}{n}$) and the variability in
    $\epsilon_{n+1}$ ($\sigma^2$).
  \item Because we estimate $\sigma^2$ the $(1-\alpha)$ prediction
    interval becomes
    \begin{displaymath}
      \bar{y}\pm t(\alpha/2;n-1)s\sqrt{1+\frac{1}{n}}
    \end{displaymath}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Evaluating a prediction interval}
  \begin{itemize}
  \item The prediction interval could be evaluated according to the
    formula.  For the clear-coat thickness data the 95\% prediction
    interval on a future thickness measurement is
  \end{itemize}
<<ccthick>>=
with(ccthickn, mean(thickness) + c(-1,1) * qt(0.975, 39) *
     sd(thickness) * sqrt(1 + 1/40))
@
  \begin{itemize}
  \item An alternative is to use the \code{predict} function applied
    to the trivial model and with the optional argument \code{interval
      = "pred"}.  This produces a matrix with n rows that are
    identical so I just look at the first row.
  \end{itemize}
<<pred>>=
predict(fm2, int = "pred")[1,]
@   
\end{frame}

\begin{frame}
  \frametitle{Tolerance intervals}
  \begin{itemize}
  \item A tolerance interval is more difficult to describe and to
    calculate than is a prediction interval.
  \item Methods for tolerance intervals are given in the text but we
    will not cover this topic in this course.
  \end{itemize}
\end{frame}

\section{5.4 Hypothesis tests}

\begin{frame}
  \frametitle{Section 5.4 Hypothesis tests}
  \begin{itemize}
  \item A hypothesis test is a procedure for deciding if a particular
    value of a parameter is reasonable, given the observed data.
  \item We have a probability model (e.g. our sample is a random
    sample from a normal distribution with mean $\mu$ and variance
    $\sigma^2$), the observed data, $y_1,y_2,\dots,y_n$, and a
    particular value of the parameter in mind (e.g. the mean
    clear-coat thickness, $\mu$, should be 65 microns).
  \item We consider two competing claims called the \Emph{null
      hypothesis}, written $H_0$, and the \Emph{alternative
      hypothesis}, written $H_a$.
  \item $H_0$ is the ``no change'' assumption.  For our example, it is
    $\mu=65$.  $H_a$ is the result we are trying to establish.  It is
    also the result indicated by the data.  In our example
    $\bar{y}=64.26$ microns.  If we are interested only in whether we
    are ``off target'' then $H_a:\mu\ne 65$.  If we are interested in
    whether the clear coats are systematically too thin then
    $H_a:\mu<65$.
  \item These are called ``two-tailed'' and ``one-tailed''
    alternatives, respectively.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{The p-value for a hypothesis test}
  \begin{itemize}
  \item We would like to establish $H_a$ directly but, because of the
    variability in the data, we can't.
  \item Instead we try to ``rule out'' $H_0$.  Again, because of the
    variability we can't rule it out completely.  What we do is to
    calculate ``the probability of seeing the data that we did, or
    something even more unusual, assuming that $H_0$ is true''.  This
    is called the \emph{p-value} for the test.
  \item Because the p-value is a probability it must be between 0 and
    1.  If the p-value is small (i.e. close to 0) we reject $H_0$ in
    favor of $H_a$.  If the p-value is not small we fail to reject $H_0$.
  \item Note that we never confirm $H_0$.  We either reject it
    (i.e. rule it out) or fail to reject it (i.e. are unable to rule
    it out).  The latter conclusion represents ``no decision''.
  \item The p-value is evaluated assuming $H_0$ is true.  The form of
    the evaluation depends on $H_a$.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Performing a hypothesis test}
  \begin{itemize}
  \item To set up a hypothesis test you must first decide on $H_0$ and
    $H_a$.  See pages 190--191 in the text for examples.  Determine if
    you want a one-sided or two-sided alternative.  If two-sided you
    are done.  If one-sided then the only claim that makes sense as
    $H_a$ is the one indicated by the data.
  \item The text describes methods based on rejection regions and
    critical values of test statistics.  These are used when you can't
    calculate probabilities for distributions like the T
    distribution.  We can do that so we use the more direct approach.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Examples 5.4.5, 5.4.7 and 5.4.10}
  \begin{itemize}
  \item The situation described in examples 5.4.5 and 5.4.7 involves
    drums of material that should have a mass of 230 kg.  A random
    sample of size 25 yields $\bar{y}=229$ and $s=4$.
  \item The sample mean
    is less than the nominal value of 230 kg.  We wish to determine if
    it is ``significantly less'' than 230 kg.  Our test is of the form
    \begin{displaymath}
      H_0:\mu=230\quad\mathrm{vs.}\quad H_a:\mu<230
    \end{displaymath}
  \item The observed t statistic, assuming that $H_0$ is true, is
    \begin{displaymath}
      t_\mathrm{obs}=\frac{229-230}{4/\sqrt{25}}=-1.25
    \end{displaymath}
  \item The probability of seeing values like this, or even more
    extreme, when $H_0$ is true, is $P(T_{24}\le-1.25)$ which we
    evaluate as \code{pt(-1.25, df = 24)} =
    \Sexpr{sprintf("%.4f",pt(-1.25, df=24))}.  This value is not
    sufficiently small for us to ``rule out'' or reject $H_0$.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Use of the R function t.test}
  \begin{itemize}
  \item The \code{t.test} function in \R{} is used for one- or
    two-sample t tests on the population mean, $\mu$.

  \item In the one-sample form you must specify the variable name as
    the first argument.  Use \code{with} or the \$ operator to access
    a variable in a data frame.

  \item You should specify the nominal value $\mu_0$ of the population
    mean as the \code{mu} argument.
    
  \item The default alternative is \code{"two.sided"}.  Specify
    \code{alt = "greater"} or \code{alt = "less"} for one-sided
    alternatives.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Clear-coat thickness data}

  The \code{ccthickn} data are from a process with a target thickness
  of 65 microns but the sample mean, $\bar{y}=64.24$ microns.  Is this
  ``significant evidence'' the $\mu<65$?
<<ccthicknttest>>=
with(ccthickn, t.test(thickness, mu = 65, alt = "less"))
@   
\end{frame}
\begin{frame}
  \frametitle{Sample sizes}
  \begin{itemize}
  \item Our conclusion in a hypothesis test is either to ``rule out''
    or reject $H_0$ in favor of $H_a$ or to fail to rule $H_0$ out.
  \item We don't know if $H_0$ is true or not.  If it is true and we
    reject it, we have made one type of ``error'' or
    mis-classification.  If it is false and we fail to reject it we
    have made another type of ``error''.  These are called Type I and
    Type II, respectively.
  \item For a test on a particular set of data the probability of a
    Type I error in rejecting $H_0$ is the p-value.  For planning
    purposes we control for this type of error by saying it should not
    exceed some value, $\alpha$.  Typically $\alpha = 0.05$ or $\alpha
    = 0.01$.
  \item To evaluate the probability of a Type II error we must specify
    a specific alternative, such as $\mu_1$, instead of a general
    alternative like $\mu\ne\mu_0$.  We also need a working value of
    $\sigma$.
  \item The \Emph{power} of a test is a function of $\mu_1$.  It is
    the probability of rejecting $H_0:\mu=\mu_0$ when $\mu=\mu_1$.
    Sometimes it is written as $1-\beta$.  In the text it is $\gamma$.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Use of power.t.test in R}
  \begin{itemize}
  \item The function \code{power.t.test} can be used to evaluate the
    power or the sample size for one- and two-sample t-tests.
  \item Two out of the three arguments \code{n}, \code{delta}
    ($=\mu_1-\mu_0$) and \code{power} must be specified and the third
    is calculated.
  \item By default \code{delta} is in standard deviation units
    (i.e. $\delta=\frac{\mu_1-\mu_0}{\sigma}$).  If you want to
    specify \code{delta} in the units of the response you should also
    give a value for the optional argument \code{sd} (= working value
    of $\sigma$).
  \item The default test type is \code{"two.sample"} (the next
    chapter).  For this chapter specify \code{type = "one.sample"}.
  \item The default alternative is \code{"two.sided"}.  For one-sided
    alternatives specify \code{alt = "one"}.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 5.4.13}
  \begin{itemize}
  \item This example involves a test of $H_0:\mu=5$ versus $H_a:\mu>5$
    (i.e. a one-sample test with a one-sided alternative) with
    $\sigma=1$, $\alpha=0.05$, $\mu_1=6$ and a power of 80\% or 0.8.
<<sampsize>>=
power.t.test(delta = 1, sd = 1, sig = 0.05, power = 0.8,
             type = "one", alt = "one")
@   
  \item We round fractional sample sizes up so we would use a sample of
    size $n=8$.  Note that the sample size calculated in this example in
    the text, assuming a ``known'' value of $\sigma$ and not an
    estimated value, is $n=7$.
  \item The values \code{sig = 0.05} and \code{sd = 1} are the
    defaults and could be omitted.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 5.4.14}
  \begin{itemize}
  \item This example is like the previous one except that the
    magnitude of $\delta=\mu_1-\mu_0$ is now 1.5 mg and the desired
    power is 90\%.
  \item The larger tolerance decreases $n$ and the higher power
    increases $n$.  The net effect is to decrease $n$.
  \end{itemize}
<<sampsize1>>=
power.t.test(del = 1.5, pow = 0.9, type = "one", alt = "one")
@   
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 5.4.16}
  \begin{itemize}
  \item As explained in the text, in practice we should use the T
    distribution and not the standard normal distribution for
    calculating sample sizes.  This is what \code{power.t.test} does.
    The text provides an alternative based on tables but they are very
    coarse.
  \item The important quantity is written in the text as
    $d=\frac{\mu-\mu_0}\sigma$.  In the \R{} function it is \code{delta}
  \item For this example $d=0.4$ with 90\% power on a one-tailed,
    one-sample test.
  \end{itemize}
<<sampsize2>>=
power.t.test(del = 0.4, pow = 0.9, typ = "one", alt = "one")
@   
\end{frame}

\section[5.5 Binomial]{5.5 Inference for Binomial Populations}
\begin{frame}
  \frametitle{Section 5.5, Inference for Binomial Populations}
  \begin{itemize}
  \item Results from a binomial simply consist of the number of
    ``successes'', $y$, and the number of trials, $n$.  This is
    already an i.i.d. sample, $\rvy_1,\rvy_2,\dots,\rvy_n$, if we
    consider each $\rvy_i$ as giving a binary (0/1) response.
  \item The parameter estimate is $\hat{p}=\frac{y}{n}$.
  \item Often approximate confidence intervals and hypothesis tests
    are formulated based on a normal distribution for $\hat{P}$ using
    a mean of $p$ and a variance of $\frac{p(1-p)}{n}$.
  \item However, we can do better.  In particular, the p-value for a
    hypothesis test can be calculated exactly.  We use
    \code{binom.test} for a one-sample binomial test and
    \code{prop.test} for comparing results from multiple
    samples. Sample sizes are calculated with \code{power.prop.test}.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 5.5.4}
  \begin{itemize}
  \item We observe $y=18$ defectives in a sample of $n=1000$ from a
    process where the nominal defect rate is 1\%.  Test $H_0:p=0.01$
    versus $H_0:p>0.01$
  \end{itemize}
<<binomtest1>>=
binom.test(18, 1000, p = 0.01, alt = "greater")
sum(dbinom(18:1000, 1000, 0.01)) # explicit P(Y >= 18) for H0 true
@   
\end{frame}
\begin{frame}[fragile]
  \frametitle{Comparison to results in the text}
  \begin{itemize}
  \item In the text the p-values for tests on a binomial distribution
    are calculated through a normal approximation and not the binomial
    distribution.
  \item This results in p-values that are lower than they should be,
    which is dangerous.  E.g. the text's p-value for Example 5.5.5 is
    0.0055 versus the exact calculation of 0.01383.  That could be
    important.
  \item Example 5.5.6 in the text gives a p-value of 0.0089 for a test
    of $H_0:p=0.05$ versus $H_a:p>0.05$ for $y=8$ and $n=72$.  The
    more accurate value is
  \end{itemize}
<<binomtest>>=
binom.test(8, 72, p = 0.05, alt = "greater")$p.value
sum(dbinom(8:72, size = 72, prob = 0.05))
@   
\end{frame}
\begin{frame}[fragile]
  \frametitle{Confidence intervals on p}
  \begin{itemize}
  \item The \code{binom.test} function produces a confidence interval
    based on ``inverting'' the exact hypothesis test.  Usually we want
    to two-sided alternative when forming a confidence interval.
  \item The \code{conf.level} argument can be used to set the desired
    confidence level (default is 95\%).
  \item These intervals are more reliable than those given in the book.
  \item Example 5.5.1 has $y=27$ out of $n=200$.  The 95\% confidence
    interval should be
<<ex551>>=
binom.test(27, 200)$conf.int
@

Notice that it is wider on the side towards $p=0.5$ than on the other
side.  This is a characteristic of the binomial.
  \end{itemize}

\end{frame}
\begin{frame}
  \frametitle{Sample sizes for tests on the binomial}
  \begin{itemize}
  \item Formulas (5.5.4), (5.5.7) and (5.5.8) in the text allow for
    calculating sample sizes for a confidence intervals with a given
    margin of error, $d$, or for tests with a null hypothesis of the
    form $H_0:p=p_0$.
  \item All of these formulas are based on the normal approximation
    and will give values that are a bit too small.
  \item In the two-sample case (next chapter) we can use the function
    \code{power.prop.test} to determine the sample size for the two
    samples.
  \end{itemize}
\end{frame}

