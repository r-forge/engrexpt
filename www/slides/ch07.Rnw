% NOTE -- ONLY EDIT THE .Rnw FILE!!!  The .tex file is,
% likely to be overwritten.
\usepackage{SweaveSlides}
\title[Chapter 7]{Chapter 7: One-factor Multi-sample Experiments}
%\subject{Models}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=10,height=6.5,strip.white=all,keep.source=TRUE}
\SweaveOpts{prefix=TRUE,prefix.string=figs/ch07,include=TRUE}
\begin{document}
\begin{frame}
  \titlepage
\end{frame}
\begin{frame}
  \frametitle{Outline}
  \tableofcontents[pausesections]
\end{frame}
<<preliminaries,echo=FALSE,results=hide>>=
options(width=60)
library(EngrExpt)
options(show.signif.stars = FALSE)
#lattice.options(default.theme = function() standard.theme(color=FALSE))
@ 
\newcommand{\rvy}{\ensuremath{\mathcal{Y}}}

\begin{frame}
  \frametitle{Factors with more than two levels}
  \begin{itemize}
  \item In the previous chapter we discussed inference for comparative
    experiments on two groups.  
  \item In chapter 3 we showed how to model a continuous response as a
    function of one or more factors that could have multiple levels.
    We used the \R{} function \code{aov} to estimate the cell means or
    the ``effects'' of the levels of each factor.
  \item The statistical assessment of whether or not the effects are
    significant is usually based on an \Emph{analysis of variance};
    hence the name \code{aov} for the model-fitting function and the
    name \code{anova} for the extractor function that produces the
    analysis of variance table.
  \item The text book emphasizes a technique called the analysis of
    means (ANOM).  This is not a widely-used technique.
  \item We will focus on the analysis of variance and another approach
    called \code{multiple comparisons} for follow-up analysis.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Overview of techniques}
  \begin{itemize}
  \item In chapter 2 we used graphical methods, such as comparative
    dotplots and comparative density plots, to display a continuous
    response as it depends on levels of a factor.  In section 4.4 we
    also discussed normal probability plots, which can be used as
    comparative plots for such data.
  \item In chapter 3 we discussed fitting models of the form
    \begin{displaymath}
      \rvy_{ij}=\mu_i+\epsilon_{ij},\quad i=1,\dots,I;\;j=1,\dots,n_i
    \end{displaymath}
    or
    \begin{displaymath}
      \rvy_{ij}=\mu+\alpha_i+\epsilon_{ij},\quad i=1,\dots,I;\;j=1,\dots,n_i
    \end{displaymath}
  \item These are the same model; just two different ways of writing
    it.  The first is called the \Emph{cell means} form and the second
    is the \Emph{effects} form.
  \item We check for differences in the mean response in two stages:
    first we check if all the means could be equal and, if we reject
    this hypothesis, we check for which levels of the factor produce
    significantly different means.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{R functions used in this chapter}
  \begin{itemize}
  \item Preliminary plots are obtained with \code{dotplot},
    \code{bwplot}, \code{densityplot} and \code{qqmath}, all in the
    comparative form.  Model fits assume that the variances in the
    groups are more-or-less equal.  Hence we check the plots for equal
    variances as well as equal means.
  \item We use \code{aov} to fit the model, \code{summary} or
    \code{anova} to obtain the analysis of variance table, and
    \code{model.tables} to obtain estimates of the cell means or the
    effects.
  \item If we reject the hypothesis $H_0:\mu_1=\mu_2=\dots=\mu_I$ (or,
    equivalently, $H_0:\alpha_1=\alpha_2=\dots=\alpha_I=0$ ) then we
    use \code{TukeyHSD} to perform multiple comparisons using Tukey's
    Honest Significant Difference method.
  \item We assess residual plots obtained with \code{plot(fm, which =
      1)} and \code{plot(fm, which = 2)}
  \end{itemize}
\end{frame}

\section{7.1 Basic Inference}
\begin{frame}
  \frametitle{Section 7.1, Basic Inference}
  \begin{itemize}
  \item We begin by plotting the data, preferably with comparative
    dotplots or comparative normal probability plots.  (The text uses
    an error-bar chart in figure 7.1 but these are less informative
    than those mentioned above.)
  \item Group means and $s^2$, the mean square error, are evaluated by
    fitting an \code{aov} model and using \code{summary} or
    \code{anova}.  The degrees of freedom for $s^2$,
    $n_1+n_2+\dots+n_I-I$, is given in the table.
  \item You could use critical values from a $T_\nu$ distribution to
    calculate confidence intervals on the individual means (p. 248)
    but the practice is discouraged.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Examples 7.1.1 and 7.1.2}
<<ex711data>>=
reac <-
    data.frame(yield = c(3.3, 4.0, 4.7,
                         4.0, 5.0, 4.3, 5.5,
                         5.3, 6.5, 6.4, 7.0, 7.7),
               cat = factor(rep(LETTERS[1:3], c(3,4,5))))
summary(fm1 <- aov(yield ~ cat, reac))
@   
\begin{center}
<<reacplot,fig=TRUE,echo=FALSE,height=4>>=
print(dotplot(cat ~ yield, reac, type = c("a","p"), pch = 21, aspect = 0.3),
      pos = c(0,0,0.65,1), more = TRUE)
print(qqmath(~ yield, reac, groups = cat, aspect = 1.3,
             xlab = "Standard normal quantiles",
             auto.key = list(columns = 3)),
      pos = c(0.65,0,1,1))
@   
\end{center}
\end{frame}

\section{7.5 Analysis of variance}

\begin{frame}[fragile]
  \frametitle{Section 7.5 Analysis of Variance}
  \begin{itemize}
  \item The F ratio quoted in the analysis of variance table is the
    ratio of $MS_\text{treatment}$ to the mean square for error,
    $MS_e$.  It is a ``signal-to-noise'' ratio based on the
    differences the differences between groups versus the differences
    within groups.
  \item Both the numerator and the denominator have degrees of freedom
    associated with them.  In this case they are $I-1$ (numerator) and
    $N-I$ (denominator) where $N$ is the total number of observations
    ($N=n_1+n_2+\dots+n_I$)
  \item The p-value is calculated from a theoretical distribution for
    this quantity, written $F_{\nu_1,\nu_2}$.  The \R{} functions for
    this distribution are \code{df}, \code{pf}, \code{qf} and \code{rf}.

  \item The hypothesis being tested is ``are all the means the same?''
    versus ``are there any differences?''.  Symbolically
    $H_0:\mu_1=\mu_2=\dots=\mu_I$, in the cell means form, or
    $H_0:\alpha_1=\alpha_2=\dots=\alpha_I=0$ in the effects form.
  \end{itemize}
\end{frame}

\section{Multiple comparisons}

\begin{frame}
  \frametitle{Multiple comparisons}
  \begin{itemize}
  \item If we reject $H_0$ in the analysis of variance the natural
    follow-up question is ``so which group means are significantly
    different''.
  \item It is tempting to use a series of t-tests to compare each pair
    of groups but doing so will inflate the probability of a false
    positive.
  \item There are several ways of controlling for this inflated false
    positive probability.  We will use Tukey's Honest Significant
    Differences, \code{TukeyHSD}, which is preferred when the groups
    are of equal importance.  (Other methods are used when we have,
    say, a control group that we wish to compare with each of several
    treatments.)
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example 7.1.1 cont'd}
<<Tukeyfm1>>=
TukeyHSD(fm1)
@   
\begin{center}
<<Tukeyplot,fig=TRUE,echo=FALSE,height=3>>=
op <- par(las = 1)
plot(TukeyHSD(fm1))
@   
\end{center}
\end{frame}

\section[7.6 Equal var]{7.6 The equal variances assumption}

\begin{frame}
  \frametitle{Section 7.6: The equal variances assumption}
  \begin{itemize}
  \item In the analysis of variance we pool the variance estimates
    from each group when evaluating the mean square for error.
  \item We are implicitly assuming that the variances of the ``noise''
    terms (the $\epsilon_{ij}$ are equal within and between groups.
  \item We should assess this in our data plots and through residual
    plots.
  \item In particular we should check the plot of the residuals versus
    the fitted values to see if the variability increases as the level
    of the response increases.  If that is the case we consider
    transformations such as $\sqrt{y_{ij}}$ or $\log(y_{ij})$ or $1/y_{ij}$.

  \item We could do formal statistical tests for equal variances but
    generally those are not powerful tests.  It is better to look for
    systematic deviations from equal variances.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Residual plots for model \code{fm1}}
  \begin{itemize}
  \item The residual plots for model \code{fm1} fit to the reactor
    data do not show alarming patterns.  There is some slight
    indication of greater variability with higher mean response by not
    definitive at all.
  \end{itemize}
  \begin{center}
<<resfm1,fig=TRUE,echo=FALSE,height=4>>=
print(xyplot(resid(fm1) ~ fitted(fm1), type = c("g","p"),
             xlab = "Fitted values", ylab = "Residuals"),
      pos = c(0,0,0.65,1), more = TRUE)
print(qqmath(~ resid(fm1), type = c("g", "p"), aspect = 1, ylab = NULL,
             xlab = "Standard normal quantiles"),
      pos = c(0.65,0,1,1))
@     
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Adding lines to a probability plot}
  \begin{itemize}
  \item Recall that, if the pattern in a normal probability plot is
    close to a straight line, the slope of the line corresponds to the
    standard deviation.
  \item In a comparative normal probability plot, equal variances
    corresponds to equal slopes of the lines.
  \item We could compare the slopes of lines fit to each group but we
    do not want to use least squares fits for this purpose.  A least
    squares fit is most sensitive to the data points at the edges and
    these are the most suspect values.
  \item There is a special type of straight line fit (based on
    quartiles) for \code{qqmath}.  You add such lines to the plot by
    redefining the \code{panel} argument.
  \end{itemize}
<<qqmathshow,eval=FALSE>>=
qqmath(~ wear, tennis, groups = type, type = c("g","p"),
       panel = function(...){
           panel.qqmath(...)
           panel.qqmathline(...)})
@   
\end{frame}

\begin{frame}[fragile]
  \frametitle{Tennis ball wear}
  \begin{center}
<<tennisplots,fig=TRUE,echo=FALSE,height=2.5>>=
print(dotplot(reorder(type, wear) ~ wear, tennis, pch = 21, type = c("p","a"),
              xlab = "Number of serves to wear off reference mark",
              jitter.y = TRUE, ylab = "Type of ball"),
      pos = c(0,0,0.7,1), more = TRUE)
print(qqmath(~ wear, tennis, groups = type, type = c("g","p"),
             auto.key = list(space = "right", points = FALSE, lines = TRUE),
             panel = function(...){panel.qqmath(...);panel.qqmathline(...)},
             aspect = 1.7, xlab = "Standard normal quantiles"),
      pos = c(0.7,0,1,1))
@ 
  \end{center}
<<tennisanova>>=
summary(fm2 <- aov(wear ~ type, tennis))
@ 
\begin{center}
<<fm2multcomp,fig=TRUE,echo=FALSE,height=3.5>>=
opar <- par(las = 1)
plot(TukeyHSD(fm2, order = TRUE))
@   
\end{center}
\end{frame}
\begin{frame}
  \frametitle{Transformation to stabilize the variance}
  \begin{itemize}
  \item When a change in variance is systematic, typically the
    variance increasing as the level of the response increases, and
    the response must be positive and the dynamic range (ratio of
    largest to smallest observation) is large then we consider
    nonlinear transformation of the response.
  \item The most common transformation is the logarithm.  Powers, such
    as the square root and the inverse are also used.
  \item Box and Cox formulated a family of transformations, indexed by
    a parameter $\lambda$ that includes powers and the logarithm.
    This family is written
    \begin{displaymath}
      y^{(\lambda)} =
      \begin{cases}
        \frac{y^\lambda - 1}{\lambda} & \lambda\ne 0\\
        \ln(y) & \lambda = 0
      \end{cases}
    \end{displaymath}
  \item There are ways to find the optimal value of $\lambda$ for a
    particular data set and model (see \code{help(boxcox, package =
      "MASS")}) but frequently we consider a sequence of possible
    transformations in the order, $\sqrt{y}$, $\log(y)$ and $1/y$.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example 7.6.4}
<<aerosol>>=
aerosol <- data.frame(cover = c(2.1,1.9,1.8,2.2,
                      4.7,3.6,3.9,3.8, 6.4,8.5,7.9,8.8),
                      brand = gl(3, 4, labels = LETTERS[1:3]))
@   
\begin{center}
<<aerosolplot,fig=TRUE,echo=FALSE,height=2.5>>=
print(dotplot(reorder(brand, cover) ~ cover, aerosol,
              pch = 21, type = c("p","a"),
              xlab = "Covering capabilities (oz./sq. ft.) of gloss white aerosol",
              jitter.y = TRUE, ylab = "Brand"),
      pos = c(0,0,0.7,1), more = TRUE)
print(qqmath(~ cover, aerosol, groups = brand, type = c("g","p"),
             auto.key = list(space = "right", points = FALSE, lines = TRUE),
             panel = function(...){panel.qqmath(...);panel.qqmathline(...)},
             aspect = 1.7, xlab = "Standard normal quantiles"),
      pos = c(0.7,0,1,1))
@ 
$\mbox{}$
<<aerosollogplot,fig=TRUE,echo=FALSE,height=2.5>>=
print(dotplot(reorder(brand, cover) ~ log(cover), aerosol,
              pch = 21, type = c("p","a"),
              xlab = "log(covering capability (oz./sq. ft.)) of gloss white aerosol",
              jitter.y = TRUE, ylab = "Brand"),
      pos = c(0,0,0.7,1), more = TRUE)
print(qqmath(~ log(cover), aerosol, groups = brand, type = c("g","p"),
             auto.key = list(space = "right", points = FALSE, lines = TRUE),
             panel = function(...){panel.qqmath(...);panel.qqmathline(...)},
             aspect = 1.7, xlab = "Standard normal quantiles"),
      pos = c(0.7,0,1,1))
@ 
\end{center}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 7.6.4 (cont'd)}
<<aerosolanova>>=
summary(fm3 <- aov(cover ~ brand, aerosol))
summary(fm3a <- aov(log(cover) ~ brand, aerosol))
TukeyHSD(fm3a)
@   
\end{frame}

\begin{frame}[fragile]
  \frametitle{Box-Cox analysis of aerosol (not required)}
  \begin{center}
<<boxcox,fig=TRUE,height=4,results=hide>>=
library(MASS)
boxcox(fm3)
@   
  \end{center}
  \begin{itemize}
  \item This plot shows that the optimal value of $\lambda$ is about
    -0.25, corresponding to the inverse fourth root but that $\lambda
    = 0$ (the logarithm) and $\lambda = -1$ (the inverse) are both
    reasonable.  $\lambda = 1$ (original scale) is not in the 95\%
    confidence interval.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Transformation of tennis example}
  \begin{itemize}
  \item Data representing counts, such as the \code{tennis} example,
    are often transformed by taking the square root.  This is because
    we expect counts to have approximately a Poisson distribution and
    the ``variance stabilizing'' transformation for the Poisson is the
    square root.
  \end{itemize}
  \begin{center}
<<tennissqrtplots,fig=TRUE,echo=FALSE,height=2.5>>=
print(dotplot(reorder(type, sqrt(wear)) ~ sqrt(wear), tennis,
              pch = 21, type = c("p","a"),
              xlab = "Square root of number of serves to wear off reference mark",
              jitter.y = TRUE, ylab = "Type of ball"),
      pos = c(0,0,0.7,1), more = TRUE)
print(qqmath(~ sqrt(wear), tennis, groups = type, type = c("g","p"),
             auto.key = list(space = "right", points = FALSE, lines = TRUE),
             panel = function(...){panel.qqmath(...);panel.qqmathline(...)},
             aspect = 1.7, xlab = "Standard normal quantiles"),
      pos = c(0.7,0,1,1))
@ 
  \end{center}
<<tennisanova>>=
summary(fm2a <- aov(sqrt(wear) ~ type, tennis))
@ 
\end{frame}
\begin{frame}[fragile]
  \frametitle{Multiple comparisons on the transformed scale}
<<tennissqrtTukey>>=
TukeyHSD(fm2a, ordered = TRUE)
@   
\begin{center}
<<tennissqrtTukeyplot,fig=TRUE,echo=FALSE,height=3>>=  
opar <- par(las = 1)
plot(TukeyHSD(fm2a, ordered = TRUE))
@ 

\end{center}
\end{frame}
\section{7.7 Sample sizes}

\begin{frame}[fragile]
  \frametitle{Section 7.7: sample sizes}
  \begin{itemize}
  \item Sample sizes or power calculations for analysis of variance
    are obtained with the \code{power.anova.test} function.  It is
    based on working values of the ``between'' and ``within''
    variances (only their ratio is important).
  \item The methods described in the text are based on the maximum
    difference in the means, standardized by the underlying $\sigma$.
    This is the TukeyHSD comparison statistic.  Some tables are given
    in Appendix B9 but they are rather sparse.
  \end{itemize}
\end{frame}
