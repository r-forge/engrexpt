% NOTE -- ONLY EDIT THE .Rnw FILE!!!  The .tex file is,
% likely to be overwritten.
\usepackage{SweaveSlides}
\title[Chapter 4]{Chapter 4: Models for the Random Error}
%\subject{Models}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=10,height=6.5,strip.white=all,keep.source=TRUE}
\SweaveOpts{prefix=TRUE,prefix.string=figs/ch04,include=TRUE}
\begin{document}
\begin{frame}
  \titlepage
\end{frame}
\begin{frame}
  \frametitle{Outline}
  \tableofcontents[pausesections]
\end{frame}
<<preliminaries,echo=FALSE,results=hide>>=
options(width=60)
library(EngrExpt)
options(show.signif.stars = FALSE)
#lattice.options(default.theme = function() standard.theme(color=FALSE))
@ 
\newcommand{\rvu}{\ensuremath{\mathcal{U}}}
\newcommand{\rvw}{\ensuremath{\mathcal{W}}}
\newcommand{\rvx}{\ensuremath{\mathcal{X}}}
\newcommand{\rvy}{\ensuremath{\mathcal{Y}}}

\section{4.1 Random variables}
\begin{frame}
  \frametitle{Random variables}
  \begin{itemize}
  \item Models for the random error are called \Emph{random
      variables}.  These are functions that map experimental outcomes
    to numeric values.  Although we don't know what the particular
    value will be, we can describe the set of all possible values and
    characterize the distribution of the random variable.
  \item Distributions are characterized by either a \Emph{probability
      density function} (pdf) or a \Emph{probability mass function}
    (pmf) depending on whether the random variable is
    \Emph{continuous} (can take on any value in an interval) or
    \Emph{discrete} (its possible values are distinct - usually
    integers representing counts).
  \item The random variable is written in upper case.  We will use a
    script font, like \rvy.  A particular value is written in
    lower case, like $y$.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Density functions for continuous random variables}
  \begin{itemize}
  \item For a continuous random variable, \rvy, the probability
    density function, written $f(y)$, is a non-negative function whose
    \Emph{integral} is unity.  That is
    \begin{displaymath}
      \begin{aligned}
        f(y)&\ge 0,\;\text{ all }y\\
        \int_{-\infty}^{\infty}f(y)\,dy&= 1
      \end{aligned}
    \end{displaymath}
    The probability that \rvy{} falls in an interval is the integral of
    the density over the interval
    \begin{displaymath}
      P[a\le\rvy\le b]= \int_a^bf(y)\,dy
    \end{displaymath}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Probability mass functions for discrete random variables}
  \begin{itemize}
  \item A discrete random variable has positive probability for only a
    finite or ``countably infinite'' set of values.  We call this set
    ``the set of all possible values'' of the random variable.
  \item The probability mass function, sometimes called more simply
    ``the probability function'' is $P(\rvy=k)=p_k$.  We require
    \begin{displaymath}
      \begin{aligned}
        p_k&\ge0,\;\text{ all }k\\
        \sum_{\text{all possible }k}p_k&=1
      \end{aligned}
    \end{displaymath}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Mean and Variance of a Random Variable}
  \begin{itemize}
  \item Just as we define the mean (average) and variance of a sample
    we have similar concepts for a random variable.
  \item The mean, written $\mu$, is a weighted average using the
    probability function or the probability density to define the
    weights.  (For a density an ``average'' is calculated by integrating.)
  \item For a discrete random variable (r.v.), the mean, or ``expected
    value'' is
    \begin{displaymath}
      \mu = \mathrm{E}(\rvy)=\sum_{\text{all possible }k}k\,P(\rvy=k)=
      \sum_kk\,p_k
    \end{displaymath}
    and the variance, or mean squared deviation, is
    \begin{displaymath}
      \mathrm{Var}(\rvy)=\sum_k\left(k-\mathrm{E}(\rvy)\right)^2P(\rvy=k)
    \end{displaymath}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Mean and variance of continuous r.v.'s}
  \begin{itemize}
  \item For a continous r.v. the mean and variance are
    \begin{displaymath}
      \begin{aligned}
        \mathrm{E}(\rvy)&=\int_{-\infty}^{\infty}y\,f(y)\,dy\\
        \mathrm{Var}(\rvy)&=\int_{-\infty}^{\infty}
        \left(y-\mathrm{E}(\rvy)\right)^2f(y)\,dy
      \end{aligned}
    \end{displaymath}
  \item Example 4.1.2 is based on a continuous uniform distribution on
    the interval $[0,4]$ with $f(y)=0.25,0\le y\le4$ and $0$
    otherwise.  Then
    \begin{displaymath}
      \mathrm{E}(\rvy)=\int_{-\infty}^\infty y\,f(y)\,dy=
      \int_0^4\frac{y}{4}\,dy=\left.\frac{y^2}{8}\right|_0^4=2
    \end{displaymath}
  \item Just as the sample mean is the point at which the sample can
    be balanced, the mean of a continuous r.v. is the point at which
    the density can be balanced.  For a symmetric density like this it
    is the point of symmetry.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Properties of expected values}
  \begin{itemize}
  \item In general we define the expected value of a function of a
    random variable, say $g(\rvy)$, written
    $\mathrm{E}\left(g(\rvy)\right)$, as either $\int_{-\infty}^\infty
    g(y)f(y)\,dy$ or $\sum_kg(k)p_k$, as appropriate.
  \item The calculation of $\mathrm{E}(\rvy)$ and $\mathrm{Var}(\rvy)$
    can get complicated.  Sometimes we can make things simpler by
    using rules related to ``linear combinations''.  For example
    \begin{displaymath}
      \mathrm{E}(a+b\rvy)=a+b\mathrm{E}(\rvy)
    \end{displaymath}
    and when we consider two random variables, say $\rvx$ and
    $\rvy$, then
    \begin{displaymath}
      \mathrm{E}(a\rvx+b\rvy)=a\mathrm{E}(\rvx)+b\mathrm{E}(\rvy)
    \end{displaymath}
    These results follow from the corresponding properties of sums or integrals.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Properties of expected values and variances}
  \begin{itemize}
  \item The expected value of a constant function, say $c$, written
    $\mathrm{E}(c)$ is always $c$, no matter what the random variable.
  \item Two random variables are said to be \Emph{independent} if the
    outcome of one does not affect the outcome of the other.
  \item The variance of a constant, $c$, is always 0 (because it is
    the expected value of $(c-c)^2$).
  \item When you multiply a random variable by $c$ you multiply its
    variance by $c^2$ (recall that the variance is on the scale of the
    square of the response).  That is,
    \begin{displaymath}
      \mathrm{Var}(c\rvy)=c^2\mathrm{Var}(\rvy)
    \end{displaymath}
  \item For independent random variables $\rvx$ and $\rvy$,
    \begin{displaymath}
      \mathrm{Var}(\rvx + \rvy) = \mathrm{Var}(\rvx) + \mathrm{Var}(\rvy)
    \end{displaymath}
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Things to watch for}
  \begin{itemize}
  \item Note that variances add, even when you subtract the random
    variables.  For independent $\rvx$ and $\rvy$
    \begin{displaymath}
      \mathrm{Var}(\rvx - \rvy) =
      \mathrm{Var}(\rvx + (-1) \rvy) =
      \mathrm{Var}(\rvx) + \mathrm{Var}(\rvy)
    \end{displaymath}

  \item Because a variance is an expected value of a square, which must be $\ge 0$,
    \begin{displaymath}
      \mathrm{Var}(\rvy)\ge 0,\quad\text{for any }\rvy
    \end{displaymath}
  \item If you evaluate a variance and it comes out negative you have
    done something wrong.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Determining probabilities}
  \begin{itemize}
  \item Probabilities can be considered as the long-term relative
    frequency of the outcomes of the experiment, assuming that it can
    be performed over and over again.
  \item In general we will refer to an experimental outcome or a set
    of outcomes as an \Emph{event}.  We use upper-case Latin letters,
    like $A$, to denote events.  We must have
    \begin{displaymath}
      0\le P(A)\le1 .
    \end{displaymath}

  \item For discrete random variables, we can evaluate $P(A)$ as the
    sum of the probabilities of individual outcomes, $p_k$, for all
    $k\in A$.  This uses the property that the probability of
    \Emph{mutually exclusive} (without overlap) events is the sum of
    the probabilities.
  \item One simple model for probabilities when there are a finite
    number of outcomes is \Emph{equally likely outcomes}.  That is,
    each of the $M$ outcomes has probability $\frac{1}{M}$.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Equally likely outcomes}
  \begin{itemize}
  \item A favorite example of equally-likely outcomes is rolling a
    die.  There are 6 possible outcomes, $1,\dots,6$ and, for a fair
    die, they are equally likely.

  \item Using this, if $A$ is the event of getting an odd number, then
    \begin{displaymath}
      P(A) = p_1 + p_3 + p_5=\frac{1}{6}+\frac{1}{6}+\frac{1}{6}=\frac{1}{2}
    \end{displaymath}

  \item Note that not all experiments with a finite set of outcomes
    have equally likely outcomes.  In fact, very few do.
  \item If you roll two dice the possible totals are $2,\dots,12$ but
    they are not equally likely, even for fair dice.  In this case
    $p_2=\frac{1}{36}$ but $p_7=\frac{1}{6}$.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Evaluating probabilities for continuous r.v.'s}
  \begin{itemize}
  \item Recall that for a continuous random variable, $\rvy$,
    \begin{displaymath}
      P(a\le\rvy\le b)=\int_a^bf(y)\,dy
    \end{displaymath}

  \item Because the probability is expressed as an integral and the
    value of the function at a single point doesn't change the value
    of the integral, we have
    \begin{displaymath}
      \begin{aligned}
        P(a\le\rvy\le b)=P(a\le\rvy<b)=P(a<\rvy\le b)=\dots
      \end{aligned}
    \end{displaymath}
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Distribution functions}
  \begin{itemize}
  \item The \Emph{cumulative distribution function} (cdf), $F(y)$, of a random
    variable, $\rvy$, is
    \begin{displaymath}
      F(y)=P(\rvy\le y)
    \end{displaymath}
    Sometimes it is called just ``the distribution function''.
  \item The cdf is defined for discrete and for continuous random variables.
  \item It is always true that
    \begin{displaymath}
      P(a<\rvy\le b)=F(b) - F(a)
    \end{displaymath}
  \item For continuous random variables you can use $\le$ in place of
    $<$ and vice versa and it is still true (see previous slide).
  \item For discrete random variables you can't interchange $\le$ and
    $<$.  You must be careful of which end points are in the interval.
  \end{itemize}
\end{frame}

\section[4.2 Discrete distributions]{4.2 Important
  Discrete Distributions}

\begin{frame}
  \frametitle{The Bernoulli Distribution}
  \begin{itemize}
  \item The Bernoulli distribution is the simplest, non-trivial
    probability distribution.  There are two possible outcomes in the
    experiment which we arbitrarily label ``success'' and ``failure''.
    $\rvy$ is the number of successes in 1 trial and must be either 0
    or 1. We write $P(\rvy=1)=p$ which implies that $P(\rvy=0)=1-p$.
  \item The value $p\in[0,1]$ is the \Emph{parameter} of this
    distribution.  (This is one of the few cases where we use a Latin
    letter, instead of a Greek letter, for a parameter.  Some texts
    use $\pi$ for this parameter but that ends up being even more
    confusing than using $p$.)
  \item We can easily establish that $\mathrm{E}(\rvy)=p$ and
    $\mathrm{Var}(\rvy)=p(1-p)$
  \item This distribution is mostly used as a building block for other
    distributions.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{The Binomial Distribution}
  \begin{itemize}
  \item If $\rvy$ is the total number of successes in $n$ independent,
    identical Bernoulli trials then we say that $\rvy$ has a binomial
    distribution with
    \begin{displaymath}
      P(\rvy=k)=p_k=\binom{n}{k}p^k(1-p)^{n-k}\quad k=0,\dots,n
    \end{displaymath}
    (Note that 0 is a possible value of $\rvy$.)
  \item The symbol $\binom{n}{k}$ denotes the \Emph{binomial coefficient}
    which has the value
    \begin{displaymath}
      \binom{n}{k}=\frac{n!}{k!(n-k)!}
    \end{displaymath}
  \item The \R{} function \code{choose} evaluates binomial coefficients.
  \end{itemize}
<<choose42>>=
choose(4, 2)
@ 
\end{frame}
\begin{frame}[fragile]
  \frametitle{R functions for the binomial}
  \begin{itemize}
  \item There are four \R{} functions associated with the binomial
    distribution: \code{dbinom} evaluates the probability function,
    \code{pbinom} evaluates the cdf, \code{qbinom} evaluates the
    quantile function (the inverse of the cdf, sort-of) and
    \code{rbinom} returns a random sample from a binomial
    distribution.
  \item Most questions on the distribution itself involve evaluating
    the probability of some event (subset of the possible values of
    $\rvy$).  I prefer to do this as \code{sum(dbinom(range, size = n,
      prob = p))}
  \item In example 4.2.2, we have a binomial with $n=50$ and $p=0.03$
    and we want $P(\rvy<2)$.  This is
  \end{itemize}
<<probylt2>>=
sum(dbinom(0:1, 50, 0.03))
@   
\end{frame}
\begin{frame}
  \frametitle{Properties of the binomial distribution}
  \begin{itemize}
  \item By considering the binomial as the sum of $n$ independent
    Bernoulli trials, each with probability $p$ of success, we can
    derive
    \begin{displaymath}
      \begin{aligned}
        \mathrm{E}(\rvy)&=np\\
        \mathrm{Var}(\rvy)&=np(1-p)
      \end{aligned}
    \end{displaymath}
  \item The formula $np$ for the expected value should make intuitive
    sense.  It says that the expected number of successes is the
    number of trials times the probability of success on each trial.
  \item Note that the formula for the variance is symmetric in $p$ and
    $1-p$.  This has to be the case because you can change the meaning
    of ``success'' and ``failure'' without changing the variability in
    the distribution.
  \item When $p$ is close to 0.5, the probability function is
    symmetric.  When $p$ is close to 0 or 1 the probability function
    is skew.
  \item For a fixed number of trials, $n$, the variance is greatest
    when $p=0.5$.  As $p$ approaches 0 or 1 the variance goes to zero.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Plotting discrete probability functions}
  \begin{itemize}
  \item One value for the \code{type} parameter in \code{xyplot}
    is \code{"h"} for ``high-density'' lines.  This is the usual way
    of plotting a probability function for a discrete random variable.
  \end{itemize}
<<binomplotdisply,eval=FALSE>>=
xyplot(dbinom(0:50, size = 50, prob = 0.5) ~ 0:50, type = "h")
@ 
  \begin{center}
<<binomplot,fig=TRUE,echo=FALSE,height=4>>=
show(xyplot(dbinom(0:50, size = 50, prob = 0.5) ~ 0:50, type = "h", xlab = NULL))
@     
  \end{center}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Interchanging ``success'' and ``failure''}
  \begin{center}
<<binomplot2,fig=TRUE,echo=FALSE>>=
show(xyplot(dbinom(0:50, 50, 0.07) + dbinom(0:50, 50, 0.93) ~ 0:50, type = "h", outer = TRUE, xlab = NULL, ylab = NULL, layout = c(1,2)))
@     
  \end{center}
\end{frame}
\begin{frame}
  \frametitle{Parameter estimation}
  \begin{itemize}
  \item When we have a probability model for some observed data we
    formulate \Emph{estimates} for the parameters using the data values.
  \item The estimates are \Emph{statistics}, which describes any value
    that you can calculate based on the data alone.  The formula for
    the estimate is called the \Emph{estimator}.
  \item For a binomial, the data are $y$, the number of successes, and
    $n$, the number of trials.  The parameter is $p$, the probability
    of success on each trial.  Not surprisingly, our ``best guess'' or
    point estimate for $p$ is the observed proportion of successes.
  \end{itemize}
  \begin{displaymath}
    \hat{p}=\frac{y}{n}
  \end{displaymath}
\end{frame}

\begin{frame}
  \frametitle{The Poisson distribution}
  \begin{itemize}
  \item Like the binomial distribution, the Poisson distribution
    models the total number of events that occur in some experiment.
  \item For the binomial distribution the events are binary responses
    in distinct trials.  The range of the random variable is $[0,n]$
    where $n$ is the total number of trials.
  \item In a \Emph{Poisson process} events occur over a continuum,
    such as time or distance or area or $\dots$.  We assume that
    \begin{enumerate}
    \item The interval of interest can be divided into small units of
      opportunity $h$ such that only one event could occur during a unit.
    \item For an opportunity unit $h$ the probability of one event is
      $\lambda h$.  The parameter $\lambda$ is called the intensity.
    \item The occurrence or non-occurence of events in non-overlapping
      intervals are independent.
    \end{enumerate}
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{The Poisson distribution (cont'd)}
  \begin{itemize}
  \item Let $\rvy$ be the number of events in an opportunity unit of
    size $t$ in a Poisson process with intensity $\lambda$.  Then
    \begin{displaymath}
      P(\rvy=k)=p_k=\frac{e^{-\lambda\,t}(\lambda\,t)^k}{k!}\quad k=1,\dots
    \end{displaymath}
  \item Most commonly $t$ is a length of time (say 1 hr.) so that
    $\lambda$ is the intensity of the events (say 30 cars/hr.).  The
    parameter has units that are counts/(units of $t$).
  \item The \R functions \code{dpois}, \code{ppois}, \code{qpois} and
    \code{rpois} take a single parameter, also called \code{lambda},
    but representing the product $\lambda\,t$ in the notation above.
    This is the expected value of $\rvy$.
  \end{itemize}
  \begin{displaymath}
    \mathrm{E}(\rvy)=\lambda\,t\quad\mathrm{Var}(\rvy)=\lambda\,t
  \end{displaymath}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 4.2.7}
  \begin{itemize}
  \item Example 4.2.6 describes a Poisson process where $\lambda=0.1$
    cars/sec.  Suppose we observe for $t=30$ seconds.  Then
    $\lambda\,t=3$ is the expected number of cars to observe.
  \item We are asked to evaluate the ``probability that no more than 2
    cars pass through the intersection'', which is $P(\rvy\le 2)$.
  \item The simple evaluation is \code{sum(dpois(0:2, lambda = 3)) =}
      \Sexpr{sprintf("%g",sum(dpois(0:2, lambda = 3)))}. An
    alternative is to use the cumulative distribution function
    \code{ppois(2, lambda = 3) =}
      \Sexpr{sprintf("%g",sum(ppois(2, lambda = 3)))}.
  \end{itemize}
<<poisplot,fig=TRUE,echo=FALSE,height=3>>=
show(xyplot(dpois(0:15,lambda=3) ~ 0:15, type = "h", xlab = NULL))
@   
\end{frame}
\begin{frame}
  \frametitle{Parameter estimation}
  \begin{itemize}
  \item Because $\mathrm{E}(\rvy)=\lambda\,t$ we estimate the
    intensity, $\lambda$, by plugging into this relationship.  That
    is, $\hat{\lambda}\,t=y$ or $\hat{\lambda}=y/t$.
  \item An interesting property of the Poisson distribution is the
    fact that the sum of independent Poisson random variables is also
    a Poisson.  (Surprisingly this doesn't happen for most other
    distributions - only for the Poisson and the Gaussian or
    ``normal'' distributions.)
  \item Because
    \begin{displaymath}
      \rvy_1\sim\mathrm{Pois}(\lambda_1t_1),\;
      \rvy_2\sim\mathrm{Pois}(\lambda_2t_2)
      \Rightarrow \rvy_1+\rvy_2\sim\mathrm{Pois}(
      \lambda_1t_1+\lambda_2t_2)
    \end{displaymath}
    we can combine the results from non-overlapping intervals when
    estimating a common intensity $\lambda$.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Relationship to binomial}
  \begin{itemize}
  \item The Poisson process can be considered as the continuous limit
    of discrete Bernoulli trials.  The usual limit processes from
    calculus apply: take more and more intervals whose width becomes
    smaller and smaller to go from the discrete jumps to the
    continuous function.
  \item When binomial probabilities needed to be evaluated from
    tables, it was important that for $n$ large and $p$ moderate (so
    that both $np$ and $n(1-p)$ become large) you could approximate
    binomial distributions by Poissons.  We set $\lambda\,t=np$.
  \item We don't need to concern ourselves with that because we can
    evaluate essentially any binomial probability.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example of Poisson approximation to binomial}
  \begin{itemize}
  \item Just for completeness we give the comparison of $P(\rvy<2)$
    for $n=50$ and $p=0.03$ versus a Poisson with $\lambda\,t=1.5$.
  \end{itemize}
<<poisapprox>>=
c(binom = sum(dbinom(0:1, 50, 0.03)), Poisson = sum(dpois(0:1, 1.5)))
all.equal(sum(dbinom(0:1, 50, 0.03)), sum(dpois(0:1, 1.5)))
@   
\end{frame}
\begin{frame}[fragile]
  \frametitle{The geometric distribution}
  \begin{itemize}
  \item For a Bernoulli process (repeated, independent trials with 0/1
    outcomes and constant probability, $p$, of ``success'') the
    binomial distribution arises from setting the number of trials to
    $n$ and counting the total number of successes.
  \item The geometric distribution is the distribution of the number
    of trials before the first success. Be aware that it can be
    defined in two ways: either count the number of trials until the
    first success or count the number of failures before the first
    success. These counts just differ by 1, of course, but you should
    be careful to check which definition you are using.
  \item The text counts the total number of trials.  The \R{}
    functions \code{dgeom}, \code{pgeom}, \code{qgeom} and
    \code{rgeom} are based on the number of failures before the first
    success.
  \item In the text's notation, if $\rvy\sim\mathrm{Geom}(p)$ then
  \end{itemize}
  \begin{displaymath}
    P(\rvy = k)=p_k=p(1-p)^{k-1}\quad k=1,2,\dots
  \end{displaymath}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 4.2.13}
  \begin{itemize}
  \item We assume that the probability of a defective is 0.03 and
    check units until a defective is found.  Let $\rvy$ be the number
    checked.  (Notice that ``success'' in this case means finding a
    defective.)
  \item The expected number of trials is $1/0.03\approx 33$.  The
    probability distribution of the number of ``failures''
    (non-defectives, in this case) before the first ``success'' is
  \end{itemize}
<<geomplot,fig=TRUE,echo=FALSE,height=4.5>>=
show(xyplot(dgeom(0:200, prob = 0.03) ~ 0:200, type = "h", xlab = NULL))
@   
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 4.2.13}
  \begin{itemize}
  \item The question in the text is whether the total number of trails is $\le 3$.
  \item In \R{} we need to check if the number of failures before the
    first success is $\le 2$.  We can use the probability mass
    function, \code{dgeom}, or the cumulative distribution function,
    \code{pgeom}
  \end{itemize}
<<geomprob>>=
sum(dgeom(0:2, prob = 0.03))
pgeom(2, prob = 0.03)
@   
\end{frame}
<<set.seed,echo=FALSE,results=hide>>=
set.seed(1234321)
@ 
\begin{frame}[fragile]
  \frametitle{Properties of the geometric distribution}
  \begin{itemize}
  \item If $\rvy\sim\mathrm{Geom}(p)$ (text's definition) then
    $\mathrm{E}(\rvy) = \frac{1}{p}$ and
    $\mathrm{Var}(\rvy)=\frac{1-p}{p^2}$
  \item In the definition used in \R,
    $\mathrm{E}(\rvy)=\frac{1}{p}-1=\frac{1-p}{p}$ and the variance is
    the same as above (why?).
  \item You can check these formulas at Wikipedia,
    \url{http://en.wikipedia.org/wiki/Geometric_distribution}.
    Information on the other distributions is also available there.
  \item You can also check it out by creating a very large random
    sample and taking the sample mean and variance.  Then change
    parameters and do it again.
  \end{itemize}
<<samplegeom,keep.source=TRUE>>=
mean(gsamp <- rgeom(1000000, prob = 0.03)) # should be near 32.3
var(gsamp)  # should be near 0.97/0.03^2 = 1077.778
@ 
\end{frame}
\begin{frame}
  \frametitle{Parameter estimation for the geometric}
  \begin{itemize}
  \item As before you must be careful if you are counting the total
    number of trials (including the last trial, which must be a
    success, by definition) or just the failures before the first success.
  \item If $y$ is the observed number of trials then $\hat{p}=1/y$.
  \item If we have several repetitions of the experiment (recommended)
    then $\hat{p}=1/\bar{y}$
  \end{itemize}
\end{frame}
\section[4.3 Continuous distributions]{4.3 Important
  Continuous Distributions}
\begin{frame}
  \frametitle{Important continuous distributions}
  \begin{itemize}
  \item We will use three basic continuous distributions: uniform,
    exponential and normal (or Gaussian) plus two derived
    distributions, the lognormal and the Weibull, which is a
    generalization of the exponential.
  \item The corresponding \R{} functions end in \code{unif},
    \code{exp}, \code{norm}, \code{lnorm} and \code{weibull}.
  \item The Wikipedia URLs end in
    \code{Uniform\_distribution\_(continuous)},
    \code{Exponential\_distribution}, \code{Normal\_distribution},
    \code{Log\_normal\_distribution} and \code{Weibull\_distribution}.

  \item Although we define the distribution by its density, in
    practice we usually work with the cumulative distribution
    function.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{The uniform distribution}
  \begin{itemize}
  \item The density is constant over a finite interval, usually
    written $[a,b]$.  Thus
    \begin{displaymath}
      f(y) =
      \begin{cases}
        \frac{1}{b-a}&a<y<b\\
        0 & \mathrm{otherwise}
      \end{cases}
    \end{displaymath}
  \item This density is symmetric about the midpoint of the interval
    so $\mathrm{E}(\rvy)=\frac{a + b}{2}$.
  \item The cumulative distribution function (cdf) is
    \begin{displaymath}
      F(y) =
      \begin{cases}
        0 & y < a\\
        \frac{y-a}{b-a}&a\le y\le b\\
        1 & y > b
      \end{cases}
    \end{displaymath}
  \item We call $\rvu\sim\mathrm{Unif}(0,1)$ the \Emph{standard
      uniform distribution}.  It is easy to verify that
    $\mathrm{Var}(\rvu)=\int_0^1\left(u-\frac{1}{2}\right)^2du=\frac{1}{12}$.
    For the general uniform distribution,
    $\mathrm{Var}(\rvy)=\frac{(b-a)^2}{12}$.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example cdf's and pdf's from uniform distributions}
  \begin{center}
<<unifcdfplot,fig=TRUE,echo=FALSE>>=
eps <- 100*.Machine$double.eps
y <- c(12.5,13-eps, 13.0,15.0,15 + eps, 15.5)
print(xyplot(dunif(y, min = 13, max = 15) + punif(y, min = 13, max = 15) ~ y,
             outer = TRUE, type = c("g","l"), xlab = "y", ylab = NULL, layout = c(1,2),
             scales = list(y = list(relation = "free"), x = list(axs = "i"))),
      pos = c(0,0,0.5,1), more = TRUE)
y <- c(-0.1,-eps, 0,1,1+eps, 1.1)
print(xyplot(dunif(y, min = 0, max = 1) + punif(y, min = 0, max = 1) ~ y,
             outer = TRUE, type = c("g","l"), xlab = "y", ylab = NULL, layout = c(1,2),
             scales = list(y = list(relation = "free"), x = list(axs = "i"))),
      pos = c(0.5,0,1,1))
@ 
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{The exponential distribution}
  \begin{itemize}
  \item A common model for lifetime data (also called \Emph{time to
      failure} or \Emph{survival analysis}) is the exponential
    distribution with density
    \begin{displaymath}
      f(y) =
      \begin{cases}
        0 & y\le 0\\
        \frac{e^{-y/\mu}}{\mu}&y > 0
      \end{cases}
    \end{displaymath}
    where $\mu>0$ is the expected time to failure.
  \item This distribution is often written in terms of the failure
    rate, $\lambda=1/\mu$, for which $f(y)=\lambda e^{-\lambda
      y},\,y>0$.
  \item The parameter $\lambda$ is the same as for the Poisson
    distribution.  Just as the geometric distribution is the number of
    trials before the first event for a Bernoulli process, the
    exponential distribution is the length of time before the first
    event in a Poisson process.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Cumulative distribution function (cdf) for exponential}
  \begin{itemize}
  \item The exponential distribution is one of the few cases where the
    cdf is easier to evaluate than the density.
    \begin{displaymath}
      F(y) =
      \int_{-\infty}^yf(u)\,du=\int_0^yf(u)\,du=1-e^{-y/\mu}\quad y>0
    \end{displaymath}

  \item This makes it particularly easy to evaluate probabilities.
    Note that the \R{} functions \code{pexp}, \code{dexp}, \code{qexp}
    and \code{rexp} use the parameter $\lambda=\frac{1}{\mu}$, not $\mu$.
  \item In Example 4.3.4 the lifetimes of compact fluorescent light
    bulbs are assumed to be exponentially with mean $\mu=7$ years.
    The probability a given bulb will last less than 3.5 yr. is
    \begin{displaymath}
      P(\rvy<3.5) = 1-e^{-3.5/7}=0.39
    \end{displaymath}
    (Recall that for continuous r.v.'s you can interchange $<$ and
    $\le$.)
    In \R{} you would use
  \end{itemize}
<<pexp>>=
pexp(3.5, rate = 1/7)
@   
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 4.3.5}
  \begin{itemize}
  \item The probability that such a light bulb will last more than 6
    years can be evaluated in two ways
<<exp6>>=
1 - pexp(6, rate = 1/7)
pexp(6, 1/7, lower = FALSE)
@
\item \code{lower = FALSE} is used with any
  cumulative probability function to obtain the probability of
  exceeding the first argument.
\item The probability that such a light bulb lasts between 3.5 and 9
  years is evaluated as the difference in the c.d.f. values.  I prefer
  to use the diff function to do this.
<<exp3.5>>=
pexp(9, 1/7) - pexp(3.5, 1/7)
diff(pexp(c(3.5, 9), 1/7))
@
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Properties and parameter estimation}
  \begin{itemize}
  \item As indicated by the notation, the expected value of the
    exponential distribution is the mean, $\mu$.  Sometimes this is
    called the \Emph{mean time between failures} or \Emph{MTBF}.
  \item If $\rvy$ is written in terms of the rate or intensity,
    $\lambda$, then $\mathrm{E}(\rvy)=\frac{1}{\lambda}$.
  \item The variance is $\mu^2$ or $\frac{1}{\lambda^2}$, hence the
    standard deviation is $\sigma=\mu$ for an exponential
    distribution.
  \item The estimate of $\mu$ is $\widehat{\mu}=\bar{y}$.
    Correspondingly, the estimate of the intensity, $\lambda$, is
    $\widehat{\lambda}=\frac{1}{\bar{y}}$.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{The normal or Gaussian distribution}
  \begin{itemize}
  \item By far the most important distribution in statistics is the
    normal or Gaussian distribution, the so-called ``bell curve''.
  \item It has two parameters, $\mu$ and $\sigma$.  These turn out to
    be the mean and the standard deviation, respectively.
  \item The density for $\rvy\sim\mathcal{N}(\mu,\sigma^2)$
    \begin{displaymath}
      f(y)=\frac{1}{\sqrt{2\pi}\sigma}
      e^{-\frac{1}{2}\left(\frac{y-\mu}{\sigma}\right)^2}
    \end{displaymath}
  \item There is no closed-form expression for the c.d.f., $F(x)$, but
    good numerical methods for its evaluation are available.
  \item The text describes methods based on converting from a general
    normal distribution to the standard normal and referring to tables
    of that distribution.  We can by-pass all that.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{The density of the standard normal distribution}
  \begin{itemize}
  \item The \Emph{standard} normal distribution is
    $\mathcal{N}(0,1)$.  That is $\mu=0$ and $\sigma=1=\sigma^2$.
  \item The parameters of the distribution are specified to the \R{}
    functions \code{dnorm}, \code{pnorm}, \code{qnorm} and
    \code{rnorm} as \code{mean} and \code{sd}.  They default to 0 and
    1, respectively.
  \end{itemize}
<<stdnormdensshow,eval=FALSE>>=
xyplot(dnorm(z) ~ z, type = c("g","l"))
@ 
  \begin{center}
<<stdnormdens,fig=TRUE,echo=FALSE,height=4>>=
z <- seq(-3.65, 3.65, 0.01)
print(xyplot(dnorm(z) ~ z, type = c("g","l"), scales = list(x = list(axs = 'i'))))
@   
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Density of $\mathcal{N}(100,15^2)$}
  The IQ scale is normalized to have a mean of 100 and a
  standard deviation of 15. 
  \begin{center}
<<IQnormdens,fig=TRUE,echo=FALSE>>=
x <- seq(47, 153, 0.5)
print(xyplot(dnorm(x, mean = 100, sd = 15) ~ x, type = c("g","l"), scales = list(x = list(axs = 'i'))))
@   
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Cdf of the Gaussian distribution}
  The cdf for a Gaussian distribution is sigmoidal (i.e. S-shaped).
<<IQnormcdf,fig=TRUE,echo=FALSE>>=
print(xyplot(pnorm(x, mean = 100, sd = 15) ~ x, type = c("g","l"), scales = list(x = list(axs = 'i'))))
@   
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 4.3.11}
  \begin{itemize}
  \item In the example the yields are distributed as
    $\mathcal{N}(450,900)$ meaning that $\mu=450$ and
    $\sigma=\sqrt{900}=30$.  To evaluate $P(\rvy<500)$, $P(\rvy>550)$
    and $P(400<\rvy<500)$ we use
  \end{itemize}
<<pnorm>>=
pnorm(500, 450, 30)
1 - pnorm(550, 450, 30)
diff(pnorm(c(400,500), 450, 30))
@   
\end{frame}
\begin{frame}[fragile]
  \frametitle{Quantiles}
  \begin{itemize}
  \item Occasionally we want to find the value of $\rvy$ with a
    specified probabilty to the left of it.  This is called a
    \Emph{quantile} of the distribution.
  \item For example, the organization called Mensa has only one
    qualification for membership - a candidate's IQ must be in the top
    2\% of the population.  If IQ scores are distributed as
    $\mathcal{N}\left(100, (15)^2\right)$ then the cutoff score is
  \end{itemize}
<<qnorm>>=
qnorm(0.98, 100, 15)
@   
\end{frame}

\begin{frame}
  \frametitle{The lognormal distribution}
  \begin{itemize}
  \item In many cases the measurements we make must be positive to be
    physically sensible.
  \item If the standard deviation is small relative to the mean (like
    the IQ scores) we can use the normal distribution as a model for
    the random variable, even though in theory it would give a (very
    small) positive probability to negative values.
  \item However, if the standard deviation is not small, compared to
    the mean, the distribution will usually be skewed.  Often the
    logarithms of the measurements are more like a normal distribution.
  \item Usually it is simplest to take the logarithms and then use the
    normal distribution.  In some cases there is interest in a model
    of the results on the original scale, for which we use the
    lognormal distribution.
  \item The parameters of the distribution, written $\mu$ and
    $\sigma$, are the mean and standard deviation of the
    \Emph{logarithm} of $\rvy$.  In the \R{} functions these are
    called \code{meanlog} and \code{sdlog}.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 4.3.20}
  The \code{alum} data are measurements of the parts per million of
  aluminum impurities in the plastics stream at a recycling plant.
<<alumdens,fig=TRUE,echo=FALSE>>=
print(densityplot(~ ppm, alum, xlab = "Parts per million"),
      split = c(1,1,1,2), more = TRUE)
print(densityplot(~ ppm, alum, scales = list(x = list(log = 2)),
                  xlab = "Parts per million (logarithmic scale)"),
      split = c(1,2,1,2))
@   
\end{frame}
\begin{frame}[fragile]
  \frametitle{Parameter estimates}
  \begin{itemize}
  \item The parameter estimates, $\widehat{\mu}$ and $\widehat\sigma$
    are the sample mean and sample standard deviation of the
    logarithms of the responses.
  \end{itemize}
<<pars>>=
with(alum, c(muhat = mean(log(ppm)), sigmahat = sd(log(ppm))))
@   
\begin{itemize}
\item Given these parameters we can evaluate probabilities either by
  first taking logarithms and using \code{pnorm} or by using
  \code{plnorm}.  If $\rvy$ is the impurity level in parts per million
  then $P(\rvy < 100)$ is
\end{itemize}
<<ex4.3.20>>=
plnorm(100, meanlog = 5.0999591, sdlog = 0.6958257)
pnorm(log(100), mean = 5.0999591, sd = 0.6958257)
@ 
\end{frame}
\begin{frame}
  \frametitle{The Weibull distribution}
  \begin{itemize}
  \item The Weibull distribution is a generalization of the
    exponential distribution and is typically used to model the time
    to failure for systems with an increasing hazard function
    (i.e. subject to wear-out failures) or with a decreasing hazard
    function (subject to ``break-in'' failures).
  \item If $\rvw$ has a Weibull distribution with scale parameter
    $\beta$ and shape parameter $\delta$ then
    \begin{displaymath}
      \rvy = \left(\frac{\rvw}{\beta}\right)^\delta
    \end{displaymath}
    has a standard exponential distribution (i.e. $\mu=1=\lambda$).
  \item From these we obtain the cdf
    \begin{displaymath}
      F(w) = 1 - e^{-(w/\beta)^\delta}
    \end{displaymath}
    which we differentiate to get the density.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Shape parameter and R functions}
  \begin{itemize}
  \item When the shape parameter, $\delta$, is 1 the Weibull is the
    same as the exponential.  With $\delta < 1$ it has a decreasing
    hazard function.  With $\delta>1$ it has an increasing hazard.
  \end{itemize}
<<weibullplot,fig=TRUE,echo=FALSE,height=6>>=
w <- seq(0, 5, 0.02)
show(xyplot(dweibull(w, 0.5, 1) + dweibull(w, 1, 1) + dweibull(w, 1.5, 1) + dweibull(w, 3, 1) ~ w,
            type = c("g","l"),
            outer = FALSE, ylab = NULL, xlab = NULL,
            auto.key = list(columns = 4, lines = TRUE, points = FALSE)))
@   
\end{frame}

\section[4.4 Probability plots]{4.4 Assessing the fit of a distribution}

\begin{frame}
  \frametitle{Assessing the fit of a distribution}
  \begin{itemize}
  \item When we propose a distribution as a model of the random error
    in measurements it is, like any statistical model, a proposed
    model and, as George Box famously said, ``All models are wrong; some
    models are useful.''
  \item We should assess if the distribution we are considering is a
    suitable model and, if not, consider alternatives.
  \item One of the most powerful methods of assessing a distribution
    or distribution family as a potential model is through
    \Emph{probability plots} in which the observed quantiles (i.e. the
    observed sample values in increasing order) are plotted versus the
    quantiles of the proposed distribution.
  \item The points form a non-decreasing pattern.  We are interested
    in whether the pattern is close to a straight line.
  \item When the distribution family incorporates only location and
    scale parameters we can use the quantiles from the standard form
    of the distribution, because we are interested only in the pattern
    and that is not changed by shifting and scaling an axis.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Probability plots (cont'd)}
  \begin{itemize}

  \item In general we need to pick a particular distribution (i.e. we
    must estimate the parameters in the distribution family) to
    evaluate the theoretical quantiles.
  \item For the exponential and normal distributions, in which the
    parameters are shift and scale parameters, we can use the standard
    form of the distribution. 
  \item Because the exponential distribution has a closed-form and
    invertible c.d.f. we can easily determine the quantiles.
  \item We plot the ordered data values, $x_{i)}, i=1,\dots,n$ versus
    $F^{-1}(\frac{i-0.05}{n})$.  Notice that we use $\frac{i-0.5}{n}$,
    not $\frac{i}{n}$.  These are the midpoints of $n$ equally spaced
    intervals from 0 to 1, sometimes called the \Emph{probability points}
  \end{itemize}
<<ppoints>>=
ppoints(20)
@   
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 4.4.1}
<<ex441>>=
cell <- c(4.23,1.89,10.52,6.46,8.32,8.6,0.41,0.91,2.66,35.71)
(mu <- mean(cell))
@   
\begin{center}
<<cellprpl,fig=TRUE,echo=FALSE,height=6>>=
print(qqmath(~ cell,
             distribution = function(x) qexp(p = x, rate = 1/mu),
             aspect = 1),
      split = c(1,1,2,1), more = TRUE)
print(qqmath(~ cell, distribution = qexp, aspect = 1),
      split = c(2,1,2,1))
@   
\end{center}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 4.41 (cont'd)}
  \begin{itemize}
  \item This example provides (probably fictitious) data on the
    lifetimes of cellular telephones.
  \item To obtain a specific exponential distribution we would
    estimate the parameter $\mu$ by the sample mean lifetime (7.971
    years) or the parameter $\lambda$, the rate, as $1/\widehat{\mu}$.
  \item The function \code{qqmath} for drawing a quantile-quantile
    plot can take a formula/data pair as the first two arguments or
    the name of a numeric variable.  An optional argument called
    \code{distribution} (the name can be abbreviated) describes the
    quantile function to use.  Typically this is just the name of a
    quantile function with the parameters at their defaults.
  \item These plots were produced by
  \end{itemize}
<<plotcalls,eval=FALSE>>=
qqmath(cell, dist = function(x) qexp(x, rate = 1/mu))
qqmath(cell, dist = qexp)
@   
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 4.4.2} Usually I set the aspect ratio of a qq
  plot to be 1 so it is easier to recognize a straight line.
<<ex442>>=
ex442 <- c(1.76,5.71,1.17,0.49,1.09,5.56,6.82,9.48,1.54,1.88)
@   
<<ex442plotshow,eval=FALSE>>=
qqmath(ex442, dist = qexp, type = c("g","p"), aspect = 1,
       xlab = "Standard exponential quantiles")
@   
\begin{center}
<<ex442plot,fig=TRUE,echo=FALSE,height=5.5>>=
show(qqmath(ex442, dist = qexp, type = c("g","p"), aspect = 1,
            xlab = "Standard exponential quantiles"))
@   
\end{center}
\end{frame}
\begin{frame}
  \begin{itemize}
  \item By far the most important probability plot is the normal
    probability plot.
  \item We use it to assess whether a set of observations appears to
    come from a normal distribution (a reasonably straight increasing
    line) or is too skewed (a ``J-shaped'' pattern) or has outliers
    (points too far down on the left or too far up on the right) or
    has ``heavy tails'' (same sort of pattern as outliers).
  \item We can use the quantiles from the standard normal distribution
    as the reference.  This is the default distribution for the
    \code{qqmath} function so we do not need to specify it.
  \item The combination of an empirical density plot and a normal
    probability plot is usually the best way to evaluate whether data
    could have come from a normal distribution.
  \item The y-intercept of the line is the sample median.  If the line
    is reasonably straight the intercept is close to the sample mean
    and the slope is close to the standard deviation.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example 4.4.3}
<<ex443,eval=FALSE>>=
len <- c(4.03,4.04,4.16,4.02,4.18,4.14,4.11,4.13,4.19,3.94,4.21,4.25)
qqmath(len, aspect = 1, type = c("g","p"), xlab = "Standard ...")
@   
\begin{center}
<<ex443plots,fig=TRUE,echo=FALSE>>=
len <- c(4.03,4.04,4.16,4.02,4.18,4.14,4.11,4.13,4.19,3.94,4.21,4.25)
print(densityplot(len, xlab = "Lengths (in.) of rubber strips"),
      pos = c(0,0,0.52,1), more = TRUE)
print(qqmath(len, aspect = 1, type = c("g","p"),
             xlab = "Standard normal quantiles"),
      pos = c(0.5,0,1,1))
@   
\end{center}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Further comments on probability plots}
  \begin{itemize}
  \item The text shows examples of fitting straight lines by least
    squares to the points on a probability plot.  Generally this is
    not considered a good idea because the points at the edges, which
    are the ones you often want to ignore, are the most influential
    points in a least squares fit.
  \item The normal probability plot is the basis for a statistical
    test of normality called the Shapiro-Wilk test implemented in the
    \code{shapiro.test} function.  A low p-value (i.e. close to zero)
    indicates ``significant'' departures from normality.
  \end{itemize}
<<shapiro>>=
shapiro.test(len)
@ 
\end{frame}
\begin{frame}[fragile]
  \frametitle{Weibull probability plots}
  \begin{itemize}
  \item The Weibull distribution family is a scale/shape family, not a
    location/scale family.
  \item We could estimate parameters for the density to create a
    quantile-quantile plot.  An example of estimating the parameters
    is given in section 4.3.  A better general method of parameter
    estimation, called ``maximum likelihood'' is implemented in the
    \code{fitdistr} function in the \code{MASS} package.
  \end{itemize}
<<brks>>=
brks <- tensile$bstrength[1:14] # stored data don't agree with text
library(MASS)
fitdistr(brks, "weibull")
@   
\end{frame}
\begin{frame}[fragile]
  \frametitle{Weibull probability plots (cont'd)} To convert the
  Weibull distribution from a scale/shape to a location/scale family
  we take a transformation.
<<wiebullshow,eval=FALSE>>=
xyplot(log(sort(brks)) ~ log(-log(1-ppoints(14))), aspect = 1)
@   
\begin{center}
<<weibullplt,fig=TRUE,echo=FALSE,height=6>>=
show(xyplot(log(sort(brks)) ~ log(-log(1-ppoints(14))),
            aspect = 1, type = c("g","p")))
@   
\end{center}
\end{frame}
