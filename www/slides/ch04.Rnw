% NOTE -- ONLY EDIT THE .Rnw FILE!!!  The .tex file is,
% likely to be overwritten.
\usepackage{SweaveSlides}
\title[Chapter 4]{Chapter 4: Models for the Random Error}
%\subject{Models}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=10,height=6.5,strip.white=all,keep.source=TRUE}
\SweaveOpts{prefix=TRUE,prefix.string=figs/ch04,include=TRUE}
\begin{document}
\begin{frame}
  \titlepage
\end{frame}
\begin{frame}
  \frametitle{Outline}
  \tableofcontents[pausesections]
\end{frame}
<<preliminaries,echo=FALSE,results=hide>>=
options(width=60)
library(EngrExpt)
options(show.signif.stars = FALSE)
#lattice.options(default.theme = function() standard.theme(color=FALSE))
@ 
\newcommand{\rvy}{\ensuremath{\mathcal{Y}}}
\newcommand{\rvx}{\ensuremath{\mathcal{X}}}

\section{4.1 Random variables}
\begin{frame}
  \frametitle{Random variables}
  \begin{itemize}
  \item Models for the random error are called \Emph{random
      variables}.  These are functions that map experimental outcomes
    to numeric values.  Although we don't know what the particular
    value will be, we can describe the set of all possible values and
    characterize the distribution of the random variable.
  \item Distributions are characterized by either a \Emph{probability
      density function} (pdf) or a \Emph{probability mass function}
    (pmf) depending on whether the random variable is
    \Emph{continuous} (can take on any value in an interval) or
    \Emph{discrete} (its possible values are distinct - usually
    integers representing counts).
  \item The random variable is written in upper case.  We will use a
    script font, like \rvy.  A particular value is written in
    lower case, like $y$.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Density functions for continuous random variables}
  \begin{itemize}
  \item For a continuous random variable, \rvy, the probability
    density function, written $f(y)$, is a non-negative function whose
    \Emph{integral} is unity.  That is
    \begin{displaymath}
      \begin{aligned}
        f(y)&\ge 0,\;\text{ all }y\\
        \int_{-\infty}^{\infty}f(y)\,dy&= 1
      \end{aligned}
    \end{displaymath}
    The probability that \rvy{} falls in an interval is the integral of
    the density over the interval
    \begin{displaymath}
      P[a\le\rvy\le b]= \int_a^bf(y)\,dy
    \end{displaymath}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Probability mass functions for discrete random variables}
  \begin{itemize}
  \item A discrete random variable has positive probability for only a
    finite or ``countably infinite'' set of values.  We call this set
    ``the set of all possible values'' of the random variable.
  \item The probability mass function, sometimes called more simply
    ``the probability function'' is $P(\rvy=k)=p_k$.  We require
    \begin{displaymath}
      \begin{aligned}
        p_k&\ge0,\;\text{ all }k\\
        \sum_{\text{all possible }k}p_k&=1
      \end{aligned}
    \end{displaymath}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Mean and Variance of a Random Variable}
  \begin{itemize}
  \item Just as we define the mean (average) and variance of a sample
    we have similar concepts for a random variable.
  \item The mean, written $\mu$, is a weighted average using the
    probability function or the probability density to define the
    weights.  (For a density an ``average'' is calculated by integrating.)
  \item For a discrete random variable (r.v.), the mean, or ``expected
    value'' is
    \begin{displaymath}
      \mu = \mathrm{E}(\rvy)=\sum_{\text{all possible }k}k\,P(\rvy=k)=
      \sum_kk\,p_k
    \end{displaymath}
    and the variance, or mean squared deviation, is
    \begin{displaymath}
      \mathrm{Var}(\rvy)=\sum_k\left(k-\mathrm{E}(\rvy)\right)^2P(\rvy=k)
    \end{displaymath}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Mean and variance of continuous r.v.'s}
  \begin{itemize}
  \item For a continous r.v. the mean and variance are
    \begin{displaymath}
      \begin{aligned}
        \mathrm{E}(\rvy)&=\int_{-\infty}^{\infty}y\,f(y)\,dy\\
        \mathrm{Var}(\rvy)&=\int_{-\infty}^{\infty}
        \left(y-\mathrm{E}(\rvy)\right)^2f(y)\,dy
      \end{aligned}
    \end{displaymath}
  \item Example 4.1.2 is based on a continuous uniform distribution on
    the interval $[0,4]$ with $f(y)=0.25,0\le y\le4$ and $0$
    otherwise.  Then
    \begin{displaymath}
      \mathrm{E}(\rvy)=\int_{-\infty}^\infty y\,f(y)\,dy=
      \int_0^4\frac{y}{4}\,dy=\left.\frac{y^2}{8}\right|_0^4=2
    \end{displaymath}
  \item Just as the sample mean is the point at which the sample can
    be balanced, the mean of a continuous r.v. is the point at which
    the density can be balanced.  For a symmetric density like this it
    is the point of symmetry.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Properties of expected values}
  \begin{itemize}
  \item In general we define the expected value of a function of a
    random variable, say $g(\rvy)$, written
    $\mathrm{E}\left(g(\rvy)\right)$, as either $\int_{-\infty}^\infty
    g(y)f(y)\,dy$ or $\sum_kg(k)p_k$, as appropriate.
  \item The calculation of $\mathrm{E}(\rvy)$ and $\mathrm{Var}(\rvy)$
    can get complicated.  Sometimes we can make things simpler by
    using rules related to ``linear combinations''.  For example
    \begin{displaymath}
      \mathrm{E}(a+b\rvy)=a+b\mathrm{E}(\rvy)
    \end{displaymath}
    and when we consider two random variables, say $\rvx$ and
    $\rvy$, then
    \begin{displaymath}
      \mathrm{E}(a\rvx+b\rvy)=a\mathrm{E}(\rvx)+b\mathrm{E}(\rvy)
    \end{displaymath}
    These results follow from the corresponding properties of sums or integrals.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Properties of expected values and variances}
  \begin{itemize}
  \item The expected value of a constant function, say $c$, written
    $\mathrm{E}(c)$ is always $c$, no matter what the random variable.
  \item Two random variables are said to be \Emph{independent} if the
    outcome of one does not affect the outcome of the other.
  \item The variance of a constant, $c$, is always 0 (because it is
    the expected value of $(c-c)^2$).
  \item When you multiply a random variable by $c$ you multiply its
    variance by $c^2$ (recall that the variance is on the scale of the
    square of the response).  That is,
    \begin{displaymath}
      \mathrm{Var}(c\rvy)=c^2\mathrm{Var}(\rvy)
    \end{displaymath}
  \item For independent random variables $\rvx$ and $\rvy$,
    \begin{displaymath}
      \mathrm{Var}(\rvx + \rvy) = \mathrm{Var}(\rvx) + \mathrm{Var}(\rvy)
    \end{displaymath}
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Things to watch for}
  \begin{itemize}
  \item Note that variances add, even when you subtract the random
    variables.  For independent $\rvx$ and $\rvy$
    \begin{displaymath}
      \mathrm{Var}(\rvx - \rvy) =
      \mathrm{Var}(\rvx + (-1) \rvy) =
      \mathrm{Var}(\rvx) + \mathrm{Var}(\rvy)
    \end{displaymath}

  \item Because a variance is an expected value of a square, which must be $\ge 0$,
    \begin{displaymath}
      \mathrm{Var}(\rvy)\ge 0,\quad\text{for any }\rvy
    \end{displaymath}
  \item If you evaluate a variance and it comes out negative you have
    done something wrong.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Determining probabilities}
  \begin{itemize}
  \item Probabilities can be considered as the long-term relative
    frequency of the outcomes of the experiment, assuming that it can
    be performed over and over again.
  \item In general we will refer to an experimental outcome or a set
    of outcomes as an \Emph{event}.  We use upper-case Latin letters,
    like $A$, to denote events.  We must have
    \begin{displaymath}
      0\le P(A)\le1 .
    \end{displaymath}

  \item For discrete random variables, we can evaluate $P(A)$ as the
    sum of the probabilities of individual outcomes, $p_k$, for all
    $k\in A$.  This uses the property that the probability of
    \Emph{mutually exclusive} (without overlap) events is the sum of
    the probabilities.
  \item One simple model for probabilities when there are a finite
    number of outcomes is \Emph{equally likely outcomes}.  That is,
    each of the $M$ outcomes has probability $\frac{1}{M}$.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Equally likely outcomes}
  \begin{itemize}
  \item A favorite example of equally-likely outcomes is rolling a
    die.  There are 6 possible outcomes, $1,\dots,6$ and, for a fair
    die, they are equally likely.

  \item Using this, if $A$ is the event of getting an odd number, then
    \begin{displaymath}
      P(A) = p_1 + p_3 + p_5=\frac{1}{6}+\frac{1}{6}+\frac{1}{6}=\frac{1}{2}
    \end{displaymath}

  \item Note that not all experiments with a finite set of outcomes
    have equally likely outcomes.  In fact, very few do.
  \item If you roll two dice the possible totals are $2,\dots,12$ but
    they are not equally likely, even for fair dice.  In this case
    $p_2=\frac{1}{36}$ but $p_7=\frac{1}{6}$.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Evaluating probabilities for continuous r.v.'s}
  \begin{itemize}
  \item Recall that for a continuous random variable, $\rvy$,
    \begin{displaymath}
      P(a\le\rvy\le b)=\int_a^bf(y)\,dy
    \end{displaymath}

  \item Because the probability is expressed as an integral and the
    value of the function at a single point doesn't change the value
    of the integral, we have
    \begin{displaymath}
      \begin{aligned}
        P(a\le\rvy\le b)=P(a\le\rvy<b)=P(a<\rvy\le b)=\dots
      \end{aligned}
    \end{displaymath}
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Distribution functions}
  \begin{itemize}
  \item The \Emph{cumulative distribution function} (cdf), $F(y)$, of a random
    variable, $\rvy$, is
    \begin{displaymath}
      F(y)=P(\rvy\le y)
    \end{displaymath}
    Sometimes it is called just ``the distribution function''.
  \item The cdf is defined for discrete and for continuous random variables.
  \item It is always true that
    \begin{displaymath}
      P(a<\rvy\le b)=F(b) - F(a)
    \end{displaymath}
  \item For continuous random variables you can use $\le$ in place of
    $<$ and vice versa and it is still true (see previous slide).
  \item For discrete random variables you can't interchange $\le$ and
    $<$.  You must be careful of which end points are in the interval.
  \end{itemize}
\end{frame}

\section[4.2 Discrete distributions]{4.2 Important
  Discrete Distributions}

\begin{frame}
  \frametitle{The Bernoulli Distribution}
  \begin{itemize}
  \item The Bernoulli distribution is the simplest, non-trivial
    probability distribution.  There are two possible outcomes in the
    experiment which we arbitrarily label ``success'' and ``failure''.
    $\rvy$ is the number of successes in 1 trial and must be either 0
    or 1. We write $P(\rvy=1)=p$ which implies that $P(\rvy=0)=1-p$.
  \item The value $p\in[0,1]$ is the \Emph{parameter} of this
    distribution.  (This is one of the few cases where we use a Latin
    letter, instead of a Greek letter, for a parameter.  Some texts
    use $\pi$ for this parameter but that ends up being even more
    confusing than using $p$.)
  \item We can easily establish that $\mathrm{E}(\rvy)=p$ and
    $\mathrm{Var}(\rvy)=p(1-p)$
  \item This distribution is mostly used as a building block for other
    distributions.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{The Binomial Distribution}
  \begin{itemize}
  \item If $\rvy$ is the total number of successes in $n$ independent,
    identical Bernoulli trials then we say that $\rvy$ has a binomial
    distribution with
    \begin{displaymath}
      P(\rvy=k)=p_k=\binom{n}{k}p^k(1-p)^{n-k}\quad k=0,\dots,n
    \end{displaymath}
    (Note that 0 is a possible value of $\rvy$.)
  \item The symbol $\binom{n}{k}$ denotes the \Emph{binomial coefficient}
    which has the value
    \begin{displaymath}
      \binom{n}{k}=\frac{n!}{k!(n-k)!}
    \end{displaymath}
  \item The \R{} function \code{choose} evaluates binomial coefficients.
  \end{itemize}
<<choose42>>=
choose(4, 2)
@ 
\end{frame}
\begin{frame}[fragile]
  \frametitle{R functions for the binomial}
  \begin{itemize}
  \item There are four \R{} functions associated with the binomial
    distribution: \code{dbinom} evaluates the probability function,
    \code{pbinom} evaluates the cdf, \code{qbinom} evaluates the
    quantile function (the inverse of the cdf, sort-of) and
    \code{rbinom} returns a random sample from a binomial
    distribution.
  \item Most questions on the distribution itself involve evaluating
    the probability of some event (subset of the possible values of
    $\rvy$).  I prefer to do this as \code{sum(dbinom(range, size = n,
      prob = p))}
  \item In example 4.2.2, we have a binomial with $n=50$ and $p=0.03$
    and we want $P(\rvy<2)$.  This is
  \end{itemize}
<<probylt2>>=
sum(dbinom(0:1, 50, 0.03))
@   
\end{frame}
\begin{frame}
  \frametitle{Properties of the binomial distribution}
  \begin{itemize}
  \item By considering the binomial as the sum of $n$ independent
    Bernoulli trials, each with probability $p$ of success, we can
    derive
    \begin{displaymath}
      \begin{aligned}
        \mathrm{E}(\rvy)&=np\\
        \mathrm{Var}(\rvy)&=np(1-p)
      \end{aligned}
    \end{displaymath}
  \item The formula $np$ for the expected value should make intuitive
    sense.  It says that the expected number of successes is the
    number of trials times the probability of success on each trial.
  \item Note that the formula for the variance is symmetric in $p$ and
    $1-p$.  This has to be the case because you can change the meaning
    of ``success'' and ``failure'' without changing the variability in
    the distribution.
  \item When $p$ is close to 0.5, the probability function is
    symmetric.  When $p$ is close to 0 or 1 the probability function
    is skew.
  \item For a fixed number of trials, $n$, the variance is greatest
    when $p=0.5$.  As $p$ approaches 0 or 1 the variance goes to zero.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Plotting discrete probability functions}
  \begin{itemize}
  \item One value for the \code{type} parameter in \code{xyplot}
    is \code{"h"} for ``high-density'' lines.  This is the usual way
    of plotting a probability function for a discrete random variable.
  \end{itemize}
<<binomplotdisply,eval=FALSE>>=
xyplot(dbinom(0:50, size = 50, prob = 0.5) ~ 0:50, type = "h")
@ 
  \begin{center}
<<binomplot,fig=TRUE,echo=FALSE,height=4>>=
show(xyplot(dbinom(0:50, size = 50, prob = 0.5) ~ 0:50, type = "h", xlab = NULL))
@     
  \end{center}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Interchanging ``success'' and ``failure''}
  \begin{center}
<<binomplot2,fig=TRUE,echo=FALSE>>=
show(xyplot(dbinom(0:50, 50, 0.07) + dbinom(0:50, 50, 0.93) ~ 0:50, type = "h", outer = TRUE, xlab = NULL, ylab = NULL, layout = c(1,2)))
@     
  \end{center}
\end{frame}
\begin{frame}
  \frametitle{Parameter estimation}
  \begin{itemize}
  \item When we have a probability model for some observed data we
    formulate \Emph{estimates} for the parameters using the data values.
  \item The estimates are \Emph{statistics}, which describes any value
    that you can calculate based on the data alone.  The formula for
    the estimate is called the \Emph{estimator}.
  \item For a binomial, the data are $y$, the number of successes, and
    $n$, the number of trials.  The parameter is $p$, the probability
    of success on each trial.  Not surprisingly, our ``best guess'' or
    point estimate for $p$ is the observed proportion of successes.
  \end{itemize}
  \begin{displaymath}
    \hat{p}=\frac{y}{n}
  \end{displaymath}
\end{frame}
